{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","import os\n","import glob\n","\n","\n","drive.mount('/content/drive')\n","os.chdir('/content/drive/MyDrive/Colab Notebooks/Close')\n","data_dir = \"Datas\"\n","test_dir = \"Test\"\n","print(\"Data klasörü var mı?\", os.path.exists(data_dir))\n","print(\"Test klasörü var mı?\", os.path.exists(test_dir))\n","data_files = glob.glob(os.path.join(data_dir, \"*.csv\"))\n","print(\"Data CSV dosyaları:\", data_files)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gJVhKVKp5XTg","executionInfo":{"status":"ok","timestamp":1749998478678,"user_tz":-180,"elapsed":2201,"user":{"displayName":"Osman Mango","userId":"07182088882667002738"}},"outputId":"a005eaac-a263-46f8-f305-9df43b31001d"},"id":"gJVhKVKp5XTg","execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Data klasörü var mı? True\n","Test klasörü var mı? True\n","Data CSV dosyaları: ['Datas/ARCLK.csv', 'Datas/ASELS.csv', 'Datas/BIMAS.csv', 'Datas/EKGYO.csv', 'Datas/EREGL.csv', 'Datas/GARAN.csv', 'Datas/FROTO.csv', 'Datas/HEKTS.csv', 'Datas/GUBRF.csv', 'Datas/KCHOL.csv', 'Datas/KOZAA.csv', 'Datas/KOZAL.csv', 'Datas/KRDMD.csv', 'Datas/MGROS.csv', 'Datas/PGSUS.csv', 'Datas/SAHOL.csv', 'Datas/PETKM.csv', 'Datas/SASA.csv', 'Datas/SOKM.csv', 'Datas/SISE.csv', 'Datas/TCELL.csv', 'Datas/THYAO.csv', 'Datas/TOASO.csv', 'Datas/TKFEN.csv', 'Datas/VAKBN.csv', 'Datas/TUPRS.csv', 'Datas/BRSAN.csv', 'Datas/YKBNK.csv', 'Datas/ALARK.csv', 'Datas/AKBNK.csv']\n"]}]},{"cell_type":"code","execution_count":null,"id":"a5d9259b-679a-49ab-bee8-619b81dbb4d8","metadata":{"id":"a5d9259b-679a-49ab-bee8-619b81dbb4d8"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler, RobustScaler\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, log_loss, mean_squared_error\n","from sklearn.linear_model import (\n","    LogisticRegression,\n","    SGDClassifier,\n","    LinearRegression,\n","    SGDRegressor,\n","    HuberRegressor,\n","    PassiveAggressiveRegressor,\n",")\n","from sklearn.multioutput import MultiOutputRegressor\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.svm import SVC\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.naive_bayes import GaussianNB\n","from imblearn.over_sampling import SMOTE\n","from typing import Tuple, List, Dict\n","import ta\n","from sklearn.model_selection import TimeSeriesSplit\n","import copy\n","from datetime import timedelta\n","from copy import deepcopy"]},{"cell_type":"code","execution_count":null,"id":"927c13e2-e24f-4e42-b843-fcd6709704a9","metadata":{"id":"927c13e2-e24f-4e42-b843-fcd6709704a9","colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"status":"ok","timestamp":1749838477656,"user_tz":-180,"elapsed":14,"user":{"displayName":"Osman Mango","userId":"07182088882667002738"}},"outputId":"f69d3b6d-4ab5-446c-bb74-ba879dd61d00"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'CONFIG = {\\n    \"window_size\": 64,\\n    \"test_size\": 64,\\n    \"data_paths\": [\"Datas/*.csv\", \"Test/*.csv\"],\\n    \"start_date\": \"2006-07-27\",\\n    \"end_date\": \"2025-05-06\",\\n    \"n_splits\": 12,\\n}'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":44}],"source":["'''CONFIG = {\n","    \"window_size\": 64,\n","    \"test_size\": 64,\n","    \"data_paths\": [\"Datas/*.csv\", \"Test/*.csv\"],\n","    \"start_date\": \"2006-07-27\",\n","    \"end_date\": \"2025-05-06\",\n","    \"n_splits\": 12,\n","}'''"]},{"cell_type":"code","execution_count":null,"id":"0ace9fc2-1b1f-48b3-824d-899e53b2a0d6","metadata":{"id":"0ace9fc2-1b1f-48b3-824d-899e53b2a0d6","colab":{"base_uri":"https://localhost:8080/","height":192},"executionInfo":{"status":"ok","timestamp":1749838490070,"user_tz":-180,"elapsed":47,"user":{"displayName":"Osman Mango","userId":"07182088882667002738"}},"outputId":"1c150d5c-f340-4b22-9081-a0e61b3b8687"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'def load_and_preprocess_all(paths: List[str]) -> Tuple[pd.DataFrame, pd.DataFrame]:\\n\\n    files = []\\n    company_index = CONFIG.get(\"company_index\", None)\\n\\n    datas_dir = \"Datas\"\\n    if not os.path.exists(datas_dir):\\n        print(f\"Directory \\'{datas_dir}\\' not found. Please ensure your data is in the correct path.\")\\n        raise FileNotFoundError(f\"Directory \\'{datas_dir}\\' not found.\")\\n\\n    for pattern in paths:\\n        matched_files = sorted(glob.glob(pattern))\\n        if company_index is not None:\\n            if len(matched_files) > company_index:\\n                files.append(matched_files[company_index])\\n            else:\\n                print(f\"Index {company_index} exceeds file count in path: {pattern}. Skipping this pattern.\")\\n        else:\\n            files.extend(matched_files)\\n\\n    if not files:\\n        raise FileNotFoundError(\"No CSV files found matching the patterns. Please check \\'data_paths\\' and \\'company_index\\'.\")\\n\\n    dfs = []\\n    for fn in files:\\n        try:\\n            df = pd.read_csv(\\n                fn, skiprows=2, header=None,\\n                names=[\"Date\", \"Close\", \"High\", \"Low\", \"Open\", \"Volume\"]\\n            )\\n            df[\"Date\"] = pd.to_datetime(df[\"Date\"], errors=\\'coerce\\', format=\\'mixed\\')\\n            df = df.dropna(subset=[\"Date\"])\\n\\n            mask = (\\n                (df[\"Date\"] >= pd.to_datetime(CONFIG[\"start_date\"])) &\\n                (df[\"Date\"] <= pd.to_datetime(CONFIG[\"end_date\"]))\\n            )\\n            df = df.loc[mask]\\n\\n            if df.empty:\\n                print(f\"DataFrame for {fn} is empty after date filtering. Skipping.\")\\n                continue\\n\\n            for col in [\"Close\", \"High\", \"Low\", \"Open\", \"Volume\"]:\\n                df[col] = pd.to_numeric(df[col], errors=\\'coerce\\')\\n\\n            df = df.dropna()\\n\\n            if df.empty:\\n                print(f\"DataFrame for {fn} is empty after NaN dropping. Skipping.\")\\n                continue\\n\\n            dfs.append(df)\\n        except Exception as e:\\n            print(f\"Error processing {fn}: {e}\")\\n\\n    if not dfs:\\n        print(\"All dataframes are empty or failed to load. Returning empty DataFrames.\")\\n        return pd.DataFrame(), pd.DataFrame()\\n\\n    df = pd.concat(dfs, ignore_index=True)\\n    df = df.sort_values(by=\"Date\")\\n    df = df.drop_duplicates(subset=\\'Date\\', keep=\\'last\\')\\n\\n    date_range = pd.date_range(start=df[\\'Date\\'].min(), end=df[\\'Date\\'].max(), freq=\\'B\\')\\n    df = df.set_index(\\'Date\\').reindex(date_range).rename_axis(\\'Date\\').reset_index()\\n\\n    df[\"Change\"] = df[\"Close\"].pct_change(fill_method=None) * 100\\n    df[\"Change\"] = df[\"Change\"].astype(float)\\n    df = df.dropna().reset_index(drop=True)\\n\\n    df[\"CDirection\"] = np.where(df[\"Change\"] < 0, 0, 1)\\n\\n    cd_count = df[\"CDirection\"].value_counts()\\n    cd_pct   = df[\"CDirection\"].value_counts(normalize=True) * 100\\n    print(f\"CDirection count: {cd_count.to_dict()}\")\\n    print(f\"CDirection % distribution: {cd_pct.round(2).to_dict()}\")\\n\\n    if len(df) >= 20:\\n        df[\"MA_20\"] = df[\"Close\"].rolling(20).mean()\\n    else:\\n        print(f\"DataFrame length ({len(df)}) is less than 20 for MA_20 calculation.\")\\n        df[\"MA_20\"] = df[\"Close\"]\\n\\n    if len(df) >= 14:\\n        df[\"RSI\"] = ta.momentum.RSIIndicator(df[\"Close\"], window=14).rsi()\\n    else:\\n        print(f\"DataFrame length ({len(df)}) is less than 14 for RSI calculation.\")\\n        df[\"RSI\"] = 50.0\\n\\n    if len(df) >= 5:\\n        df[\"Volatility\"] = df[\"Close\"].pct_change().rolling(5).std()\\n    else:\\n        print(f\"DataFrame length ({len(df)}) is less than 5 for Volatility calculation.\")\\n        df[\"Volatility\"] = 0.01\\n\\n    df = df.dropna().reset_index(drop=True)\\n\\n    if df.empty:\\n        print(\"DataFrame is empty after feature engineering and dropping NaNs. Cannot proceed.\")\\n        return pd.DataFrame(), pd.DataFrame()\\n\\n    window_size = CONFIG[\"window_size\"]\\n    test_size = CONFIG[\"test_size\"]\\n\\n    if len(df) < window_size + test_size:\\n        print(f\"DataFrame length ({len(df)}) is very small for window_size ({window_size}). \"\\n                       f\"Consider increasing data duration. Current length: {len(df)}\")\\n        test_size = max(int(len(df) * 0.2), 1)\\n\\n    if len(df) >= window_size + test_size:\\n        df_test = df.iloc[-(test_size):].copy()\\n        df_train = df.iloc[:-(test_size)]\\n    else:\\n        print(f\"DataFrame length ({len(df)}) is smaller than window_size+test_size. Using fallback split.\")\\n        split_idx = max(len(df) - test_size, window_size)\\n        df_train = df.iloc[:split_idx].copy()\\n        df_test = df.iloc[split_idx:].copy()\\n\\n    if df_train.empty:\\n        print(\"Training DataFrame is empty after splitting. Adjust \\'start_date\\' or data length.\")\\n        return pd.DataFrame(), pd.DataFrame()\\n\\n    df_test\\n    df_train\\n\\n    return df_train, df_test'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":45}],"source":["'''def load_and_preprocess_all(paths: List[str]) -> Tuple[pd.DataFrame, pd.DataFrame]:\n","\n","    files = []\n","    company_index = CONFIG.get(\"company_index\", None)\n","\n","    datas_dir = \"Datas\"\n","    if not os.path.exists(datas_dir):\n","        print(f\"Directory '{datas_dir}' not found. Please ensure your data is in the correct path.\")\n","        raise FileNotFoundError(f\"Directory '{datas_dir}' not found.\")\n","\n","    for pattern in paths:\n","        matched_files = sorted(glob.glob(pattern))\n","        if company_index is not None:\n","            if len(matched_files) > company_index:\n","                files.append(matched_files[company_index])\n","            else:\n","                print(f\"Index {company_index} exceeds file count in path: {pattern}. Skipping this pattern.\")\n","        else:\n","            files.extend(matched_files)\n","\n","    if not files:\n","        raise FileNotFoundError(\"No CSV files found matching the patterns. Please check 'data_paths' and 'company_index'.\")\n","\n","    dfs = []\n","    for fn in files:\n","        try:\n","            df = pd.read_csv(\n","                fn, skiprows=2, header=None,\n","                names=[\"Date\", \"Close\", \"High\", \"Low\", \"Open\", \"Volume\"]\n","            )\n","            df[\"Date\"] = pd.to_datetime(df[\"Date\"], errors='coerce', format='mixed')\n","            df = df.dropna(subset=[\"Date\"])\n","\n","            mask = (\n","                (df[\"Date\"] >= pd.to_datetime(CONFIG[\"start_date\"])) &\n","                (df[\"Date\"] <= pd.to_datetime(CONFIG[\"end_date\"]))\n","            )\n","            df = df.loc[mask]\n","\n","            if df.empty:\n","                print(f\"DataFrame for {fn} is empty after date filtering. Skipping.\")\n","                continue\n","\n","            for col in [\"Close\", \"High\", \"Low\", \"Open\", \"Volume\"]:\n","                df[col] = pd.to_numeric(df[col], errors='coerce')\n","\n","            df = df.dropna()\n","\n","            if df.empty:\n","                print(f\"DataFrame for {fn} is empty after NaN dropping. Skipping.\")\n","                continue\n","\n","            dfs.append(df)\n","        except Exception as e:\n","            print(f\"Error processing {fn}: {e}\")\n","\n","    if not dfs:\n","        print(\"All dataframes are empty or failed to load. Returning empty DataFrames.\")\n","        return pd.DataFrame(), pd.DataFrame()\n","\n","    df = pd.concat(dfs, ignore_index=True)\n","    df = df.sort_values(by=\"Date\")\n","    df = df.drop_duplicates(subset='Date', keep='last')\n","\n","    date_range = pd.date_range(start=df['Date'].min(), end=df['Date'].max(), freq='B')\n","    df = df.set_index('Date').reindex(date_range).rename_axis('Date').reset_index()\n","\n","    df[\"Change\"] = df[\"Close\"].pct_change(fill_method=None) * 100\n","    df[\"Change\"] = df[\"Change\"].astype(float)\n","    df = df.dropna().reset_index(drop=True)\n","\n","    df[\"CDirection\"] = np.where(df[\"Change\"] < 0, 0, 1)\n","\n","    cd_count = df[\"CDirection\"].value_counts()\n","    cd_pct   = df[\"CDirection\"].value_counts(normalize=True) * 100\n","    print(f\"CDirection count: {cd_count.to_dict()}\")\n","    print(f\"CDirection % distribution: {cd_pct.round(2).to_dict()}\")\n","\n","    if len(df) >= 20:\n","        df[\"MA_20\"] = df[\"Close\"].rolling(20).mean()\n","    else:\n","        print(f\"DataFrame length ({len(df)}) is less than 20 for MA_20 calculation.\")\n","        df[\"MA_20\"] = df[\"Close\"]\n","\n","    if len(df) >= 14:\n","        df[\"RSI\"] = ta.momentum.RSIIndicator(df[\"Close\"], window=14).rsi()\n","    else:\n","        print(f\"DataFrame length ({len(df)}) is less than 14 for RSI calculation.\")\n","        df[\"RSI\"] = 50.0\n","\n","    if len(df) >= 5:\n","        df[\"Volatility\"] = df[\"Close\"].pct_change().rolling(5).std()\n","    else:\n","        print(f\"DataFrame length ({len(df)}) is less than 5 for Volatility calculation.\")\n","        df[\"Volatility\"] = 0.01\n","\n","    df = df.dropna().reset_index(drop=True)\n","\n","    if df.empty:\n","        print(\"DataFrame is empty after feature engineering and dropping NaNs. Cannot proceed.\")\n","        return pd.DataFrame(), pd.DataFrame()\n","\n","    window_size = CONFIG[\"window_size\"]\n","    test_size = CONFIG[\"test_size\"]\n","\n","    if len(df) < window_size + test_size:\n","        print(f\"DataFrame length ({len(df)}) is very small for window_size ({window_size}). \"\n","                       f\"Consider increasing data duration. Current length: {len(df)}\")\n","        test_size = max(int(len(df) * 0.2), 1)\n","\n","    if len(df) >= window_size + test_size:\n","        df_test = df.iloc[-(test_size):].copy()\n","        df_train = df.iloc[:-(test_size)]\n","    else:\n","        print(f\"DataFrame length ({len(df)}) is smaller than window_size+test_size. Using fallback split.\")\n","        split_idx = max(len(df) - test_size, window_size)\n","        df_train = df.iloc[:split_idx].copy()\n","        df_test = df.iloc[split_idx:].copy()\n","\n","    if df_train.empty:\n","        print(\"Training DataFrame is empty after splitting. Adjust 'start_date' or data length.\")\n","        return pd.DataFrame(), pd.DataFrame()\n","\n","    df_test\n","    df_train\n","\n","    return df_train, df_test'''"]},{"cell_type":"code","source":["'''def dater(df_train, df_test):\n","    df_train[\"Date\"] = pd.to_datetime(df_train[\"Date\"])\n","    df_train[\"Year\"] = df_train[\"Date\"].dt.year\n","    df_train[\"Month\"] = df_train[\"Date\"].dt.month\n","    df_train[\"Day\"] = df_train[\"Date\"].dt.day\n","    df_train[\"Weekday\"] = df_train[\"Date\"].dt.weekday\n","    df_train = df_train.drop(columns=[\"Date\", \"Change\"], errors=\"ignore\")\n","\n","    X_train = df_train.drop(\"CDirection\", axis=1)\n","    y_train = df_train[\"CDirection\"]\n","\n","\n","    df_test[\"Date\"] = pd.to_datetime(df_test[\"Date\"])\n","    df_test[\"Year\"] = df_test[\"Date\"].dt.year\n","    df_test[\"Month\"] = df_test[\"Date\"].dt.month\n","    df_test[\"Day\"] = df_test[\"Date\"].dt.day\n","    df_test[\"Weekday\"] = df_test[\"Date\"].dt.weekday\n","\n","    actual_cdirection = df_test[\"CDirection\"].reset_index(drop=True) if \"CDirection\" in df_test.columns else None\n","    test_dates = df_test[\"Date\"].reset_index(drop=True)\n","    X_test = df_test.drop(columns=[\"Date\", \"Change\", \"CDirection\"], errors=\"ignore\")\n","\n","    return X_train, y_train, actual_cdirection, test_dates, X_test'''"],"metadata":{"id":"dVpPrAq4Ebhx","colab":{"base_uri":"https://localhost:8080/","height":192},"executionInfo":{"status":"ok","timestamp":1749838497911,"user_tz":-180,"elapsed":7,"user":{"displayName":"Osman Mango","userId":"07182088882667002738"}},"outputId":"e9eb7187-87f2-444e-81c5-020860e57b0d"},"id":"dVpPrAq4Ebhx","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'def dater(df_train, df_test):\\n    df_train[\"Date\"] = pd.to_datetime(df_train[\"Date\"])\\n    df_train[\"Year\"] = df_train[\"Date\"].dt.year\\n    df_train[\"Month\"] = df_train[\"Date\"].dt.month\\n    df_train[\"Day\"] = df_train[\"Date\"].dt.day\\n    df_train[\"Weekday\"] = df_train[\"Date\"].dt.weekday\\n    df_train = df_train.drop(columns=[\"Date\", \"Change\"], errors=\"ignore\")\\n\\n    X_train = df_train.drop(\"CDirection\", axis=1)\\n    y_train = df_train[\"CDirection\"]\\n\\n\\n    df_test[\"Date\"] = pd.to_datetime(df_test[\"Date\"])\\n    df_test[\"Year\"] = df_test[\"Date\"].dt.year\\n    df_test[\"Month\"] = df_test[\"Date\"].dt.month\\n    df_test[\"Day\"] = df_test[\"Date\"].dt.day\\n    df_test[\"Weekday\"] = df_test[\"Date\"].dt.weekday\\n\\n    actual_cdirection = df_test[\"CDirection\"].reset_index(drop=True) if \"CDirection\" in df_test.columns else None\\n    test_dates = df_test[\"Date\"].reset_index(drop=True)\\n    X_test = df_test.drop(columns=[\"Date\", \"Change\", \"CDirection\"], errors=\"ignore\")\\n\\n    return X_train, y_train, actual_cdirection, test_dates, X_test'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":46}]},{"cell_type":"code","source":["'''def DRpredictor():\n","\n","    df_train_raw, df_test = load_and_preprocess_all(CONFIG[\"data_paths\"])\n","\n","\n","    X_train, y_train, actual_cdirection, test_dates, X_test = dater(df_train_raw, df_test)\n","\n","\n","    tscv = TimeSeriesSplit(n_splits=CONFIG[\"n_splits\"])\n","    all_results = {}\n","\n","\n","    for fold, (train_idx, val_idx) in enumerate(tscv.split(X_train), 1):\n","        X_t, X_v = X_train.iloc[train_idx], X_train.iloc[val_idx]\n","        y_t, y_v = y_train.iloc[train_idx], y_train.iloc[val_idx]\n","\n","\n","        scaler = StandardScaler()\n","        X_t_scaled = scaler.fit_transform(X_t)\n","        X_v_scaled = scaler.transform(X_v)\n","\n","\n","        clf = LogisticRegression(random_state=42)\n","        clf.fit(X_t_scaled, y_t)\n","\n","\n","        tr_loss = log_loss(y_t, clf.predict_proba(X_t_scaled))\n","        vl_loss = log_loss(y_v, clf.predict_proba(X_v_scaled))\n","        print(f\"Fold {fold} — Train Loss: {tr_loss:.4f}, Val Loss: {vl_loss:.4f}\")\n","\n","\n","        all_results[fold] = {\n","            'model': clf,\n","            'scaler': scaler,\n","            'val_loss': vl_loss\n","        }\n","\n","\n","    best_fold = min(all_results, key=lambda f: all_results[f]['val_loss'])\n","    info = all_results[best_fold]\n","    print(f\"\\nEn iyi fold: {best_fold} — Val Loss = {info['val_loss']:.4f}\")\n","\n","\n","    best_clf = info['model']\n","    best_scaler = info['scaler']\n","\n","\n","    print(\"\\n=== Test Aşaması ===\")\n","    X_test_scaled = best_scaler.transform(X_test)\n","    y_pred = best_clf.predict(X_test_scaled)\n","\n","\n","    df_result = pd.DataFrame({\n","        'Date': test_dates,\n","        'Real_CDirection': actual_cdirection,\n","        'Predicted_CDirection': y_pred\n","    })\n","\n","    print(f\"Toplam {len(y_pred)} tahmin yapıldı: {df_result['Date'].iloc[0]} → {df_result['Date'].iloc[-1]}\")\n","    return df_result'''"],"metadata":{"id":"xHjI4vo5DJXQ","executionInfo":{"status":"ok","timestamp":1749838507469,"user_tz":-180,"elapsed":11,"user":{"displayName":"Osman Mango","userId":"07182088882667002738"}},"colab":{"base_uri":"https://localhost:8080/","height":192},"outputId":"3201238d-8291-48e3-9c97-e849cb688bad"},"id":"xHjI4vo5DJXQ","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'def DRpredictor():\\n\\n    df_train_raw, df_test = load_and_preprocess_all(CONFIG[\"data_paths\"])\\n\\n\\n    X_train, y_train, actual_cdirection, test_dates, X_test = dater(df_train_raw, df_test)\\n\\n\\n    tscv = TimeSeriesSplit(n_splits=CONFIG[\"n_splits\"])\\n    all_results = {}\\n\\n\\n    for fold, (train_idx, val_idx) in enumerate(tscv.split(X_train), 1):\\n        X_t, X_v = X_train.iloc[train_idx], X_train.iloc[val_idx]\\n        y_t, y_v = y_train.iloc[train_idx], y_train.iloc[val_idx]\\n\\n\\n        scaler = StandardScaler()\\n        X_t_scaled = scaler.fit_transform(X_t)\\n        X_v_scaled = scaler.transform(X_v)\\n\\n\\n        clf = LogisticRegression(random_state=42)\\n        clf.fit(X_t_scaled, y_t)\\n\\n\\n        tr_loss = log_loss(y_t, clf.predict_proba(X_t_scaled))\\n        vl_loss = log_loss(y_v, clf.predict_proba(X_v_scaled))\\n        print(f\"Fold {fold} — Train Loss: {tr_loss:.4f}, Val Loss: {vl_loss:.4f}\")\\n\\n\\n        all_results[fold] = {\\n            \\'model\\': clf,\\n            \\'scaler\\': scaler,\\n            \\'val_loss\\': vl_loss\\n        }\\n\\n\\n    best_fold = min(all_results, key=lambda f: all_results[f][\\'val_loss\\'])\\n    info = all_results[best_fold]\\n    print(f\"\\nEn iyi fold: {best_fold} — Val Loss = {info[\\'val_loss\\']:.4f}\")\\n\\n\\n    best_clf = info[\\'model\\']\\n    best_scaler = info[\\'scaler\\']\\n\\n\\n    print(\"\\n=== Test Aşaması ===\")\\n    X_test_scaled = best_scaler.transform(X_test)\\n    y_pred = best_clf.predict(X_test_scaled)\\n\\n\\n    df_result = pd.DataFrame({\\n        \\'Date\\': test_dates,\\n        \\'Real_CDirection\\': actual_cdirection,\\n        \\'Predicted_CDirection\\': y_pred\\n    })\\n\\n    print(f\"Toplam {len(y_pred)} tahmin yapıldı: {df_result[\\'Date\\'].iloc[0]} → {df_result[\\'Date\\'].iloc[-1]}\")\\n    return df_result'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":47}]},{"cell_type":"code","source":["'''def DRpredictor():\n","\n","    df_train_raw, df_test = load_and_preprocess_all(CONFIG[\"data_paths\"])\n","    X_train_orig, y_train_orig, actual_cdirection, test_dates, X_test = dater(df_train_raw, df_test)\n","\n","\n","    tscv = TimeSeriesSplit(n_splits=CONFIG[\"n_splits\"])\n","    all_results = {}\n","    for fold, (tr_idx, val_idx) in enumerate(tscv.split(X_train_orig), 1):\n","        X_t, X_v = X_train_orig.iloc[tr_idx], X_train_orig.iloc[val_idx]\n","        y_t, y_v = y_train_orig.iloc[tr_idx], y_train_orig.iloc[val_idx]\n","\n","        scaler = StandardScaler()\n","        X_t_s = scaler.fit_transform(X_t)\n","        X_v_s = scaler.transform(X_v)\n","\n","        clf = LogisticRegression(random_state=42)\n","        clf.fit(X_t_s, y_t)\n","        vl_loss = log_loss(y_v, clf.predict_proba(X_v_s))\n","\n","        all_results[fold] = {\"model\": clf, \"scaler\": scaler, \"val_loss\": vl_loss}\n","\n","    best_fold = min(all_results, key=lambda f: all_results[f][\"val_loss\"])\n","    print(f\"\\nEn iyi fold: {best_fold} — Val Loss = {all_results[best_fold]['val_loss']:.4f}\")\n","\n","\n","\n","\n","    X_train_wf = X_train_orig.copy()\n","    y_train_wf = y_train_orig.copy()\n","\n","    results = {\"Date\": [], \"Real_CDirection\": [], \"Predicted_CDirection\": [], \"Error_Rate\": []}\n","\n","    wrong_count = 0\n","\n","    for i in range(len(X_test)):\n","\n","        scaler = StandardScaler()\n","        X_tr_s = scaler.fit_transform(X_train_wf)\n","        clf = LogisticRegression(random_state=42)\n","        clf.fit(X_tr_s, y_train_wf)\n","\n","\n","        x_i = X_test.iloc[[i]]\n","        x_i_s = scaler.transform(x_i)\n","        y_hat = clf.predict(x_i_s)[0]\n","        y_true = actual_cdirection.iloc[i]\n","        dt = test_dates.iloc[i]\n","\n","        if y_hat != y_true:\n","            wrong_count += 1\n","        error_rate = (wrong_count / (i + 1)) * 100\n","\n","\n","        results[\"Date\"].append(dt)\n","        results[\"Real_CDirection\"].append(y_true)\n","        results[\"Predicted_CDirection\"].append(y_hat)\n","        results[\"Error_Rate\"].append(error_rate)\n","\n","\n","        X_train_wf = pd.concat([X_train_wf, x_i], ignore_index=True)\n","        y_train_wf = pd.concat([y_train_wf, pd.Series([y_true])], ignore_index=True)\n","\n","\n","    df_result = pd.DataFrame(results)\n","    print(f\"\\nWalk-forward ile toplam {len(df_result)} tahmin yapıldı: \"\n","          f\"{df_result['Date'].iloc[0]} → {df_result['Date'].iloc[-1]}\")\n","    return df_result'''\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":192},"id":"5usPrAm0oH39","executionInfo":{"status":"ok","timestamp":1749838517167,"user_tz":-180,"elapsed":66,"user":{"displayName":"Osman Mango","userId":"07182088882667002738"}},"outputId":"b4792f90-376f-4e09-98a8-932e18c83efe"},"id":"5usPrAm0oH39","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'def DRpredictor():\\n\\n    df_train_raw, df_test = load_and_preprocess_all(CONFIG[\"data_paths\"])\\n    X_train_orig, y_train_orig, actual_cdirection, test_dates, X_test = dater(df_train_raw, df_test)\\n\\n\\n    tscv = TimeSeriesSplit(n_splits=CONFIG[\"n_splits\"])\\n    all_results = {}\\n    for fold, (tr_idx, val_idx) in enumerate(tscv.split(X_train_orig), 1):\\n        X_t, X_v = X_train_orig.iloc[tr_idx], X_train_orig.iloc[val_idx]\\n        y_t, y_v = y_train_orig.iloc[tr_idx], y_train_orig.iloc[val_idx]\\n\\n        scaler = StandardScaler()\\n        X_t_s = scaler.fit_transform(X_t)\\n        X_v_s = scaler.transform(X_v)\\n\\n        clf = LogisticRegression(random_state=42)\\n        clf.fit(X_t_s, y_t)\\n        vl_loss = log_loss(y_v, clf.predict_proba(X_v_s))\\n\\n        all_results[fold] = {\"model\": clf, \"scaler\": scaler, \"val_loss\": vl_loss}\\n\\n    best_fold = min(all_results, key=lambda f: all_results[f][\"val_loss\"])\\n    print(f\"\\nEn iyi fold: {best_fold} — Val Loss = {all_results[best_fold][\\'val_loss\\']:.4f}\")\\n\\n\\n\\n\\n    X_train_wf = X_train_orig.copy()\\n    y_train_wf = y_train_orig.copy()\\n\\n    results = {\"Date\": [], \"Real_CDirection\": [], \"Predicted_CDirection\": [], \"Error_Rate\": []}\\n\\n    wrong_count = 0\\n\\n    for i in range(len(X_test)):\\n\\n        scaler = StandardScaler()\\n        X_tr_s = scaler.fit_transform(X_train_wf)\\n        clf = LogisticRegression(random_state=42)\\n        clf.fit(X_tr_s, y_train_wf)\\n\\n\\n        x_i = X_test.iloc[[i]]\\n        x_i_s = scaler.transform(x_i)\\n        y_hat = clf.predict(x_i_s)[0]\\n        y_true = actual_cdirection.iloc[i]\\n        dt = test_dates.iloc[i]\\n\\n        if y_hat != y_true:\\n            wrong_count += 1\\n        error_rate = (wrong_count / (i + 1)) * 100\\n\\n\\n        results[\"Date\"].append(dt)\\n        results[\"Real_CDirection\"].append(y_true)\\n        results[\"Predicted_CDirection\"].append(y_hat)\\n        results[\"Error_Rate\"].append(error_rate)\\n\\n\\n        X_train_wf = pd.concat([X_train_wf, x_i], ignore_index=True)\\n        y_train_wf = pd.concat([y_train_wf, pd.Series([y_true])], ignore_index=True)\\n\\n\\n    df_result = pd.DataFrame(results)\\n    print(f\"\\nWalk-forward ile toplam {len(df_result)} tahmin yapıldı: \"\\n          f\"{df_result[\\'Date\\'].iloc[0]} → {df_result[\\'Date\\'].iloc[-1]}\")\\n    return df_result'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":48}]},{"cell_type":"code","source":["'''def DRpredictor():\n","\n","    df_train_raw, df_test = load_and_preprocess_all(CONFIG[\"data_paths\"])\n","    X_train_orig, y_train_orig, actual_cdirection, test_dates, X_test = dater(df_train_raw, df_test)\n","\n","\n","    tscv = TimeSeriesSplit(n_splits=CONFIG[\"n_splits\"])\n","    cv_losses = []\n","    for tr_idx, val_idx in tscv.split(X_train_orig):\n","        X_t, X_v = X_train_orig.iloc[tr_idx], X_train_orig.iloc[val_idx]\n","        y_t, y_v = y_train_orig.iloc[tr_idx], y_train_orig.iloc[val_idx]\n","        scaler_cv = StandardScaler().fit(X_t)\n","        clf_cv    = LogisticRegression(random_state=42).fit(\n","                        scaler_cv.transform(X_t), y_t\n","                    )\n","        cv_losses.append(log_loss(y_v, clf_cv.predict_proba(scaler_cv.transform(X_v))))\n","    best_fold = int(pd.Series(cv_losses).idxmin()) + 1\n","    print(f\"En iyi fold (bilgi amaçlı): {best_fold} — Val Loss = {cv_losses[best_fold-1]:.4f}\")\n","\n","\n","    final_scaler = StandardScaler().fit(X_train_orig)\n","    final_clf    = LogisticRegression(random_state=42).fit(\n","                       final_scaler.transform(X_train_orig),\n","                       y_train_orig\n","                   )\n","\n","\n","    X_train_wf = X_train_orig.copy()\n","    y_train_wf = y_train_orig.copy()\n","    wrong_count = 0\n","\n","    results = {\n","        \"Date\": [],\n","        \"Real_CDirection\": [],\n","        \"Predicted_CDirection\": [],\n","        \"Error_Rate\": []\n","    }\n","\n","\n","    for i in range(len(X_test)):\n","\n","        x_i    = X_test.iloc[[i]]\n","        x_i_s  = final_scaler.transform(x_i)\n","        y_hat  = final_clf.predict(x_i_s)[0]\n","        y_true = actual_cdirection.iloc[i]\n","        dt     = test_dates.iloc[i]\n","\n","\n","        if y_hat != y_true:\n","            wrong_count += 1\n","        error_rate = wrong_count / (i + 1) * 100\n","\n","\n","        results[\"Date\"].append(dt)\n","        results[\"Real_CDirection\"].append(y_true)\n","        results[\"Predicted_CDirection\"].append(y_hat)\n","        results[\"Error_Rate\"].append(error_rate)\n","\n","\n","        X_train_wf = pd.concat([X_train_wf, x_i], ignore_index=True)\n","        y_train_wf = pd.concat([y_train_wf, pd.Series([y_true])], ignore_index=True)\n","\n","\n","    df_result = pd.DataFrame(results)\n","    print(f\"\\nWalk-forward süreci tamamlandı: {len(df_result)} adımdan \"\n","          f\"{df_result['Date'].iloc[0]} → {df_result['Date'].iloc[-1]}\")\n","    return df_result'''\n"],"metadata":{"id":"k0OhDxASrVrv","executionInfo":{"status":"ok","timestamp":1749838527207,"user_tz":-180,"elapsed":25,"user":{"displayName":"Osman Mango","userId":"07182088882667002738"}},"colab":{"base_uri":"https://localhost:8080/","height":192},"outputId":"3b85fd9f-c380-4d29-81ca-2a443bf8aec0"},"id":"k0OhDxASrVrv","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'def DRpredictor():\\n\\n    df_train_raw, df_test = load_and_preprocess_all(CONFIG[\"data_paths\"])\\n    X_train_orig, y_train_orig, actual_cdirection, test_dates, X_test = dater(df_train_raw, df_test)\\n\\n\\n    tscv = TimeSeriesSplit(n_splits=CONFIG[\"n_splits\"])\\n    cv_losses = []\\n    for tr_idx, val_idx in tscv.split(X_train_orig):\\n        X_t, X_v = X_train_orig.iloc[tr_idx], X_train_orig.iloc[val_idx]\\n        y_t, y_v = y_train_orig.iloc[tr_idx], y_train_orig.iloc[val_idx]\\n        scaler_cv = StandardScaler().fit(X_t)\\n        clf_cv    = LogisticRegression(random_state=42).fit(\\n                        scaler_cv.transform(X_t), y_t\\n                    )\\n        cv_losses.append(log_loss(y_v, clf_cv.predict_proba(scaler_cv.transform(X_v))))\\n    best_fold = int(pd.Series(cv_losses).idxmin()) + 1\\n    print(f\"En iyi fold (bilgi amaçlı): {best_fold} — Val Loss = {cv_losses[best_fold-1]:.4f}\")\\n\\n\\n    final_scaler = StandardScaler().fit(X_train_orig)\\n    final_clf    = LogisticRegression(random_state=42).fit(\\n                       final_scaler.transform(X_train_orig),\\n                       y_train_orig\\n                   )\\n\\n\\n    X_train_wf = X_train_orig.copy()\\n    y_train_wf = y_train_orig.copy()\\n    wrong_count = 0\\n\\n    results = {\\n        \"Date\": [],\\n        \"Real_CDirection\": [],\\n        \"Predicted_CDirection\": [],\\n        \"Error_Rate\": []\\n    }\\n\\n\\n    for i in range(len(X_test)):\\n\\n        x_i    = X_test.iloc[[i]]\\n        x_i_s  = final_scaler.transform(x_i)\\n        y_hat  = final_clf.predict(x_i_s)[0]\\n        y_true = actual_cdirection.iloc[i]\\n        dt     = test_dates.iloc[i]\\n\\n\\n        if y_hat != y_true:\\n            wrong_count += 1\\n        error_rate = wrong_count / (i + 1) * 100\\n\\n\\n        results[\"Date\"].append(dt)\\n        results[\"Real_CDirection\"].append(y_true)\\n        results[\"Predicted_CDirection\"].append(y_hat)\\n        results[\"Error_Rate\"].append(error_rate)\\n\\n\\n        X_train_wf = pd.concat([X_train_wf, x_i], ignore_index=True)\\n        y_train_wf = pd.concat([y_train_wf, pd.Series([y_true])], ignore_index=True)\\n\\n\\n    df_result = pd.DataFrame(results)\\n    print(f\"\\nWalk-forward süreci tamamlandı: {len(df_result)} adımdan \"\\n          f\"{df_result[\\'Date\\'].iloc[0]} → {df_result[\\'Date\\'].iloc[-1]}\")\\n    return df_result'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":49}]},{"cell_type":"code","source":["'''result_dir = \"Results/Change\"\n","os.makedirs(result_dir, exist_ok=True)\n","\n","\n","company_files = sorted(glob.glob(\"Datas/*.csv\"))\n","\n","for idx, filepath in enumerate(company_files):\n","\n","    company_code = os.path.splitext(os.path.basename(filepath))[0]\n","    print(f\"\\n=== Processing [{idx}] {company_code} ===\")\n","\n","\n","    CONFIG[\"company_index\"] = idx\n","\n","\n","    result_df = DRpredictor()\n","\n","\n","    result_path = f\"{result_dir}/{company_code}_results.csv\"\n","    result_df.to_csv(result_path, index=False)\n","    print(f\"→ Saved: {result_path}\")'''"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":105},"id":"Rd0Clt7cDNEg","executionInfo":{"status":"ok","timestamp":1749838537894,"user_tz":-180,"elapsed":16,"user":{"displayName":"Osman Mango","userId":"07182088882667002738"}},"outputId":"97f13ac0-0c81-4ec3-fe3b-a58151ff3839"},"id":"Rd0Clt7cDNEg","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'result_dir = \"Results/Change\"\\nos.makedirs(result_dir, exist_ok=True)\\n\\n\\ncompany_files = sorted(glob.glob(\"Datas/*.csv\"))\\n\\nfor idx, filepath in enumerate(company_files):\\n\\n    company_code = os.path.splitext(os.path.basename(filepath))[0]\\n    print(f\"\\n=== Processing [{idx}] {company_code} ===\")\\n\\n\\n    CONFIG[\"company_index\"] = idx\\n\\n\\n    result_df = DRpredictor()\\n\\n\\n    result_path = f\"{result_dir}/{company_code}_results.csv\"\\n    result_df.to_csv(result_path, index=False)\\n    print(f\"→ Saved: {result_path}\")'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":50}]},{"cell_type":"code","source":["base_dir = \"Results/Change\"\n","methods = [\"Vanilla\", \"Retrain\", \"Stepper\", \"Beast\"]\n","\n","summary = []\n","\n","for method in methods:\n","    method_dir = os.path.join(base_dir, method)\n","    files = sorted(glob.glob(os.path.join(method_dir, \"*_results.csv\")))\n","\n","\n","    total_wrong = 0\n","    total_count = 0\n","\n","    per_company = []\n","\n","    for fn in files:\n","        df = pd.read_csv(fn)\n","\n","        wrong = (df[\"Real_CDirection\"] != df[\"Predicted_CDirection\"]).sum()\n","        count = len(df)\n","        err_rate = wrong / count * 100\n","\n","        total_wrong += wrong\n","        total_count += count\n","\n","        company_code = os.path.splitext(os.path.basename(fn))[0].replace(\"_results\", \"\")\n","        per_company.append({\n","            \"Method\": method,\n","            \"Company\": company_code,\n","            \"Error_Rate (%)\": err_rate\n","        })\n","\n","\n","    overall_err = total_wrong / total_count * 100 if total_count else None\n","    summary.append({\n","        \"Method\": method,\n","        \"Overall_Error_Rate (%)\": overall_err\n","    })\n","\n","\n","    df_pc = pd.DataFrame(per_company).set_index([\"Method\", \"Company\"])\n","    print(f\"\\n=== {method} — Şirket Bazında Hata Oranları ===\")\n","    print(df_pc.to_string())\n","\n","\n","df_summary = pd.DataFrame(summary).set_index(\"Method\")\n","print(\"\\n=== Yöntem Geneli Hata Oranları ===\")\n","print(df_summary.to_string())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wUlijPrjzdQC","executionInfo":{"status":"ok","timestamp":1749761478656,"user_tz":-180,"elapsed":399,"user":{"displayName":"Osman Mango","userId":"07182088882667002738"}},"outputId":"5a59dce6-f09c-4021-a67b-b16bb64cfb1e"},"id":"wUlijPrjzdQC","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","=== Vanilla — Şirket Bazında Hata Oranları ===\n","                 Error_Rate (%)\n","Method  Company                \n","Vanilla AKBNK           20.3125\n","        ALARK           23.4375\n","        ARCLK           17.1875\n","        ASELS           23.4375\n","        BIMAS           18.7500\n","        BRSAN           17.1875\n","        EKGYO            9.3750\n","        EREGL           39.0625\n","        FROTO           65.6250\n","        GARAN           12.5000\n","        GUBRF           14.0625\n","        HEKTS           43.7500\n","        KCHOL           25.0000\n","        KOZAA           12.5000\n","        KOZAL           23.4375\n","        KRDMD           28.1250\n","        MGROS           17.1875\n","        PETKM           15.6250\n","        PGSUS           45.3125\n","        SAHOL           20.3125\n","        SASA             7.8125\n","        SISE            28.1250\n","        SOKM            14.0625\n","        TCELL           14.0625\n","        THYAO           20.3125\n","        TKFEN           32.8125\n","        TOASO           15.6250\n","        TUPRS           35.9375\n","        VAKBN           18.7500\n","        YKBNK           20.3125\n","\n","=== Retrain — Şirket Bazında Hata Oranları ===\n","                 Error_Rate (%)\n","Method  Company                \n","Retrain AKBNK           18.7500\n","        ALARK           23.4375\n","        ARCLK           21.8750\n","        ASELS           17.1875\n","        BIMAS           17.1875\n","        BRSAN           34.3750\n","        EKGYO            9.3750\n","        EREGL           29.6875\n","        FROTO           54.6875\n","        GARAN           15.6250\n","        GUBRF           28.1250\n","        HEKTS           39.0625\n","        KCHOL           18.7500\n","        KOZAA           10.9375\n","        KOZAL           17.1875\n","        KRDMD           18.7500\n","        MGROS           17.1875\n","        PETKM           18.7500\n","        PGSUS           14.0625\n","        SAHOL           14.0625\n","        SASA            37.5000\n","        SISE            25.0000\n","        SOKM            18.7500\n","        TCELL           20.3125\n","        THYAO           18.7500\n","        TKFEN           39.0625\n","        TOASO           15.6250\n","        TUPRS           28.1250\n","        VAKBN           20.3125\n","        YKBNK           18.7500\n","\n","=== Stepper — Şirket Bazında Hata Oranları ===\n","                 Error_Rate (%)\n","Method  Company                \n","Stepper AKBNK           23.4375\n","        ALARK           23.4375\n","        ARCLK           21.8750\n","        ASELS           15.6250\n","        BIMAS           17.1875\n","        BRSAN           34.3750\n","        EKGYO            9.3750\n","        EREGL           29.6875\n","        FROTO           59.3750\n","        GARAN           12.5000\n","        GUBRF           28.1250\n","        HEKTS           42.1875\n","        KCHOL           18.7500\n","        KOZAA           10.9375\n","        KOZAL           17.1875\n","        KRDMD           20.3125\n","        MGROS           17.1875\n","        PETKM           17.1875\n","        PGSUS           14.0625\n","        SAHOL           14.0625\n","        SASA            39.0625\n","        SISE            26.5625\n","        SOKM            18.7500\n","        TCELL           15.6250\n","        THYAO           18.7500\n","        TKFEN           35.9375\n","        TOASO           14.0625\n","        TUPRS           29.6875\n","        VAKBN           20.3125\n","        YKBNK           18.7500\n","\n","=== Beast — Şirket Bazında Hata Oranları ===\n","                Error_Rate (%)\n","Method Company                \n","Beast  AKBNK          7.575758\n","       ALARK          4.545455\n","       ARCLK         12.121212\n","       ASELS          9.090909\n","       BIMAS          9.090909\n","       BRSAN          4.545455\n","       EKGYO         16.666667\n","       EREGL         12.121212\n","       FROTO         15.151515\n","       GARAN         13.636364\n","       GUBRF          7.575758\n","       HEKTS         12.121212\n","       KCHOL          7.575758\n","       KOZAA          3.030303\n","       KOZAL          7.575758\n","       KRDMD         10.606061\n","       MGROS         10.606061\n","       PETKM         10.606061\n","       PGSUS          7.575758\n","       SAHOL          9.090909\n","       SASA           9.090909\n","       SISE          13.636364\n","       SOKM           7.575758\n","       TCELL          6.060606\n","       THYAO         12.121212\n","       TKFEN         21.212121\n","       TOASO          7.575758\n","       TUPRS          7.575758\n","       VAKBN         13.636364\n","       YKBNK          7.575758\n","\n","=== Yöntem Geneli Hata Oranları ===\n","         Overall_Error_Rate (%)\n","Method                         \n","Vanilla               23.333333\n","Retrain               22.708333\n","Stepper               22.812500\n","Beast                  9.898990\n"]}]},{"cell_type":"code","source":["'''import os\n","import glob\n","import numpy as np\n","import pandas as pd\n","from typing import List, Tuple\n","from sklearn.linear_model import SGDClassifier\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import TimeSeriesSplit\n","from sklearn.metrics import log_loss\n","from datetime import timedelta\n","\n","\n","CONFIG = {\n","    \"window_size\": 64,\n","    \"test_size\": 64,\n","    \"data_paths\": [\"Datas/*.csv\", \"Test/*.csv\"],\n","    \"start_date\": \"2006-07-27\",\n","    \"end_date\": \"2025-05-06\",\n","    \"n_splits\": 12,\n","}\n","\n","def load_and_preprocess_single(file_path: str) -> pd.DataFrame:\n","    try:\n","        df = pd.read_csv(\n","            file_path,\n","            skiprows=2,\n","            header=None,\n","            names=[\"Date\", \"Close\", \"High\", \"Low\", \"Open\", \"Volume\"]\n","        )\n","\n","\n","        df[\"Date\"] = pd.to_datetime(df[\"Date\"], errors='coerce')\n","        df = df.dropna(subset=[\"Date\"])\n","\n","\n","        end_date = pd.to_datetime(CONFIG[\"end_date\"]) + timedelta(days=1)\n","        mask = (df[\"Date\"] >= pd.to_datetime(CONFIG[\"start_date\"])) & (df[\"Date\"] <= end_date)\n","        df = df.loc[mask]\n","\n","        if df.empty:\n","            print(f\"Uyarı: {os.path.basename(file_path)} dosyası filtreleme sonrası boş\")\n","            return pd.DataFrame()\n","\n","\n","        for col in [\"Close\", \"High\", \"Low\", \"Open\", \"Volume\"]:\n","            df[col] = pd.to_numeric(df[col], errors='coerce')\n","\n","        return df.dropna()\n","\n","    except Exception as e:\n","        print(f\"Hata: {file_path} yüklenirken - {str(e)}\")\n","        return pd.DataFrame()\n","\n","def load_company_data(company_code: str) -> Tuple[pd.DataFrame, pd.DataFrame]:\n","\n","    train_path = f\"Datas/{company_code}.csv\"\n","    test_path = f\"Test/{company_code}.csv\"\n","\n","\n","    train_df = load_and_preprocess_single(train_path)\n","    test_df = load_and_preprocess_single(test_path)\n","\n","    if train_df.empty or test_df.empty:\n","        return pd.DataFrame(), pd.DataFrame()\n","\n","\n","    def calculate_features(df):\n","        df = df.copy()\n","        df[\"Change\"] = df[\"Close\"].pct_change(fill_method=None) * 100\n","        df[\"CDirection\"] = np.where(df[\"Change\"] < 0, 0, 1)\n","\n","\n","        df[\"MA_20\"] = df[\"Close\"].rolling(20, min_periods=1).mean()\n","\n","\n","        rsi = ta.momentum.RSIIndicator(df[\"Close\"], window=14).rsi()\n","        df[\"RSI\"] = rsi.fillna(50)\n","\n","\n","        volatility = df[\"Close\"].pct_change().rolling(5, min_periods=1).std()\n","        df[\"Volatility\"] = volatility.fillna(0.01)\n","\n","        return df.dropna()\n","\n","\n","    train_df = calculate_features(train_df)\n","    test_df = calculate_features(test_df)\n","\n","    return train_df, test_df\n","\n","def dater(df_train: pd.DataFrame, df_test: pd.DataFrame) -> tuple:\n","    def process_date_features(df):\n","        df = df.copy()\n","        df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n","        df[\"Year\"] = df[\"Date\"].dt.year\n","        df[\"Month\"] = df[\"Date\"].dt.month\n","        df[\"Day\"] = df[\"Date\"].dt.day\n","        df[\"Weekday\"] = df[\"Date\"].dt.weekday\n","        return df\n","\n","    df_train = process_date_features(df_train)\n","    df_test = process_date_features(df_test)\n","\n","\n","    y_train = df_train[\"CDirection\"]\n","    X_train = df_train.drop([\"Date\", \"Change\", \"CDirection\"], axis=1, errors=\"ignore\")\n","\n","\n","    actual_cdirection = df_test[\"CDirection\"].reset_index(drop=True)\n","    test_dates = df_test[\"Date\"].reset_index(drop=True)\n","    X_test = df_test.drop([\"Date\", \"Change\", \"CDirection\"], axis=1, errors=\"ignore\")\n","\n","    return X_train, y_train, actual_cdirection, test_dates, X_test\n","\n","def DRpredictor(company_code: str) -> pd.DataFrame:\n","\n","    train_df, test_df = load_company_data(company_code)\n","\n","    if train_df.empty or test_df.empty:\n","        print(f\"{company_code} için yeterli veri yok. Atlanıyor.\")\n","        return pd.DataFrame()\n","\n","    X_train, y_train, actual_cdirection, test_dates, X_test = dater(train_df, test_df)\n","\n","\n","    tscv = TimeSeriesSplit(n_splits=CONFIG[\"n_splits\"])\n","    best_val_loss = float('inf')\n","    best_scaler = None\n","    best_model = None\n","\n","    for fold, (train_idx, val_idx) in enumerate(tscv.split(X_train), 1):\n","\n","        X_t, X_v = X_train.iloc[train_idx], X_train.iloc[val_idx]\n","        y_t, y_v = y_train.iloc[train_idx], y_train.iloc[val_idx]\n","\n","\n","        scaler = StandardScaler()\n","        X_t_scaled = scaler.fit_transform(X_t)\n","        X_v_scaled = scaler.transform(X_v)\n","\n","\n","        model = SGDClassifier(\n","            loss='log_loss',\n","            learning_rate='adaptive',\n","            eta0=0.01,\n","            max_iter=1000,\n","            tol=1e-3,\n","            random_state=42,\n","            early_stopping=True\n","        )\n","        model.fit(X_t_scaled, y_t)\n","\n","\n","        val_loss = log_loss(y_v, model.predict_proba(X_v_scaled))\n","\n","\n","        if val_loss < best_val_loss:\n","            best_val_loss = val_loss\n","            best_scaler = scaler\n","            best_model = model\n","\n","    print(f\"\\nEn iyi model val_loss: {best_val_loss:.4f}\")\n","\n","\n","    results = {\n","        \"Date\": [],\n","        \"Real_CDirection\": [],\n","        \"Predicted_CDirection\": [],\n","        \"Error_Rate\": [],\n","        \"Cumulative_Accuracy\": []\n","    }\n","\n","    correct_count = 0\n","    X_walk = X_train.copy()\n","    y_walk = y_train.copy()\n","\n","\n","    dynamic_model = SGDClassifier(\n","        loss='log_loss',\n","        learning_rate='adaptive',\n","        eta0=0.01,\n","        max_iter=1000,\n","        warm_start=True,\n","        random_state=42\n","    )\n","\n","\n","    X_walk_scaled = best_scaler.transform(X_walk)\n","    dynamic_model.fit(X_walk_scaled, y_walk)\n","\n","    for i in range(len(X_test)):\n","\n","        x_i = X_test.iloc[[i]]\n","        x_i_scaled = best_scaler.transform(x_i)\n","\n","\n","        y_pred = dynamic_model.predict(x_i_scaled)[0]\n","        y_true = actual_cdirection.iloc[i]\n","        current_date = test_dates.iloc[i]\n","\n","\n","        correct = y_pred == y_true\n","        correct_count = correct_count + 1 if correct else correct_count\n","        cumulative_accuracy = correct_count / (i + 1)\n","\n","\n","        results[\"Date\"].append(current_date)\n","        results[\"Real_CDirection\"].append(y_true)\n","        results[\"Predicted_CDirection\"].append(y_pred)\n","        results[\"Error_Rate\"].append(1 - cumulative_accuracy)\n","        results[\"Cumulative_Accuracy\"].append(cumulative_accuracy)\n","\n","\n","        X_walk = pd.concat([X_walk, x_i], ignore_index=True)\n","        y_walk = pd.concat([y_walk, pd.Series([y_true])], ignore_index=True)\n","\n","\n","        if (i + 1) % 5 == 0:\n","            X_walk_scaled = best_scaler.transform(X_walk)\n","            dynamic_model.fit(X_walk_scaled, y_walk)\n","\n","\n","    result_df = pd.DataFrame(results)\n","    print(f\"\\n{company_code} için walk-forward tamamlandı. Son doğruluk: {cumulative_accuracy:.2%}\")\n","    return result_df'''"],"metadata":{"id":"FOygEqci4wAW","executionInfo":{"status":"ok","timestamp":1749838577543,"user_tz":-180,"elapsed":90,"user":{"displayName":"Osman Mango","userId":"07182088882667002738"}},"colab":{"base_uri":"https://localhost:8080/","height":192},"outputId":"214181a9-b01a-4775-e2e3-55222997e3e4"},"id":"FOygEqci4wAW","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'import os\\nimport glob\\nimport numpy as np\\nimport pandas as pd\\nfrom typing import List, Tuple\\nfrom sklearn.linear_model import SGDClassifier\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.model_selection import TimeSeriesSplit\\nfrom sklearn.metrics import log_loss\\nfrom datetime import timedelta\\n\\n\\nCONFIG = {\\n    \"window_size\": 64,\\n    \"test_size\": 64,\\n    \"data_paths\": [\"Datas/*.csv\", \"Test/*.csv\"],\\n    \"start_date\": \"2006-07-27\",\\n    \"end_date\": \"2025-05-06\",\\n    \"n_splits\": 12,\\n}\\n\\ndef load_and_preprocess_single(file_path: str) -> pd.DataFrame:\\n    try:\\n        df = pd.read_csv(\\n            file_path,\\n            skiprows=2,\\n            header=None,\\n            names=[\"Date\", \"Close\", \"High\", \"Low\", \"Open\", \"Volume\"]\\n        )\\n\\n\\n        df[\"Date\"] = pd.to_datetime(df[\"Date\"], errors=\\'coerce\\')\\n        df = df.dropna(subset=[\"Date\"])\\n\\n\\n        end_date = pd.to_datetime(CONFIG[\"end_date\"]) + timedelta(days=1)\\n        mask = (df[\"Date\"] >= pd.to_datetime(CONFIG[\"start_date\"])) & (df[\"Date\"] <= end_date)\\n        df = df.loc[mask]\\n\\n        if df.empty:\\n            print(f\"Uyarı: {os.path.basename(file_path)} dosyası filtreleme sonrası boş\")\\n            return pd.DataFrame()\\n\\n\\n        for col in [\"Close\", \"High\", \"Low\", \"Open\", \"Volume\"]:\\n            df[col] = pd.to_numeric(df[col], errors=\\'coerce\\')\\n\\n        return df.dropna()\\n\\n    except Exception as e:\\n        print(f\"Hata: {file_path} yüklenirken - {str(e)}\")\\n        return pd.DataFrame()\\n\\ndef load_company_data(company_code: str) -> Tuple[pd.DataFrame, pd.DataFrame]:\\n\\n    train_path = f\"Datas/{company_code}.csv\"\\n    test_path = f\"Test/{company_code}.csv\"\\n\\n\\n    train_df = load_and_preprocess_single(train_path)\\n    test_df = load_and_preprocess_single(test_path)\\n\\n    if train_df.empty or test_df.empty:\\n        return pd.DataFrame(), pd.DataFrame()\\n\\n\\n    def calculate_features(df):\\n        df = df.copy()\\n        df[\"Change\"] = df[\"Close\"].pct_change(fill_method=None) * 100\\n        df[\"CDirection\"] = np.where(df[\"Change\"] < 0, 0, 1)\\n\\n\\n        df[\"MA_20\"] = df[\"Close\"].rolling(20, min_periods=1).mean()\\n\\n\\n        rsi = ta.momentum.RSIIndicator(df[\"Close\"], window=14).rsi()\\n        df[\"RSI\"] = rsi.fillna(50)\\n\\n\\n        volatility = df[\"Close\"].pct_change().rolling(5, min_periods=1).std()\\n        df[\"Volatility\"] = volatility.fillna(0.01)\\n\\n        return df.dropna()\\n\\n\\n    train_df = calculate_features(train_df)\\n    test_df = calculate_features(test_df)\\n\\n    return train_df, test_df\\n\\ndef dater(df_train: pd.DataFrame, df_test: pd.DataFrame) -> tuple:\\n    def process_date_features(df):\\n        df = df.copy()\\n        df[\"Date\"] = pd.to_datetime(df[\"Date\"])\\n        df[\"Year\"] = df[\"Date\"].dt.year\\n        df[\"Month\"] = df[\"Date\"].dt.month\\n        df[\"Day\"] = df[\"Date\"].dt.day\\n        df[\"Weekday\"] = df[\"Date\"].dt.weekday\\n        return df\\n\\n    df_train = process_date_features(df_train)\\n    df_test = process_date_features(df_test)\\n\\n\\n    y_train = df_train[\"CDirection\"]\\n    X_train = df_train.drop([\"Date\", \"Change\", \"CDirection\"], axis=1, errors=\"ignore\")\\n\\n\\n    actual_cdirection = df_test[\"CDirection\"].reset_index(drop=True)\\n    test_dates = df_test[\"Date\"].reset_index(drop=True)\\n    X_test = df_test.drop([\"Date\", \"Change\", \"CDirection\"], axis=1, errors=\"ignore\")\\n\\n    return X_train, y_train, actual_cdirection, test_dates, X_test\\n\\ndef DRpredictor(company_code: str) -> pd.DataFrame:\\n\\n    train_df, test_df = load_company_data(company_code)\\n\\n    if train_df.empty or test_df.empty:\\n        print(f\"{company_code} için yeterli veri yok. Atlanıyor.\")\\n        return pd.DataFrame()\\n\\n    X_train, y_train, actual_cdirection, test_dates, X_test = dater(train_df, test_df)\\n\\n\\n    tscv = TimeSeriesSplit(n_splits=CONFIG[\"n_splits\"])\\n    best_val_loss = float(\\'inf\\')\\n    best_scaler = None\\n    best_model = None\\n\\n    for fold, (train_idx, val_idx) in enumerate(tscv.split(X_train), 1):\\n\\n        X_t, X_v = X_train.iloc[train_idx], X_train.iloc[val_idx]\\n        y_t, y_v = y_train.iloc[train_idx], y_train.iloc[val_idx]\\n\\n\\n        scaler = StandardScaler()\\n        X_t_scaled = scaler.fit_transform(X_t)\\n        X_v_scaled = scaler.transform(X_v)\\n\\n\\n        model = SGDClassifier(\\n            loss=\\'log_loss\\',\\n            learning_rate=\\'adaptive\\',\\n            eta0=0.01,\\n            max_iter=1000,\\n            tol=1e-3,\\n            random_state=42,\\n            early_stopping=True\\n        )\\n        model.fit(X_t_scaled, y_t)\\n\\n\\n        val_loss = log_loss(y_v, model.predict_proba(X_v_scaled))\\n\\n\\n        if val_loss < best_val_loss:\\n            best_val_loss = val_loss\\n            best_scaler = scaler\\n            best_model = model\\n\\n    print(f\"\\nEn iyi model val_loss: {best_val_loss:.4f}\")\\n\\n\\n    results = {\\n        \"Date\": [],\\n        \"Real_CDirection\": [],\\n        \"Predicted_CDirection\": [],\\n        \"Error_Rate\": [],\\n        \"Cumulative_Accuracy\": []\\n    }\\n\\n    correct_count = 0\\n    X_walk = X_train.copy()\\n    y_walk = y_train.copy()\\n\\n\\n    dynamic_model = SGDClassifier(\\n        loss=\\'log_loss\\',\\n        learning_rate=\\'adaptive\\',\\n        eta0=0.01,\\n        max_iter=1000,\\n        warm_start=True,\\n        random_state=42\\n    )\\n\\n\\n    X_walk_scaled = best_scaler.transform(X_walk)\\n    dynamic_model.fit(X_walk_scaled, y_walk)\\n\\n    for i in range(len(X_test)):\\n\\n        x_i = X_test.iloc[[i]]\\n        x_i_scaled = best_scaler.transform(x_i)\\n\\n\\n        y_pred = dynamic_model.predict(x_i_scaled)[0]\\n        y_true = actual_cdirection.iloc[i]\\n        current_date = test_dates.iloc[i]\\n\\n\\n        correct = y_pred == y_true\\n        correct_count = correct_count + 1 if correct else correct_count\\n        cumulative_accuracy = correct_count / (i + 1)\\n\\n\\n        results[\"Date\"].append(current_date)\\n        results[\"Real_CDirection\"].append(y_true)\\n        results[\"Predicted_CDirection\"].append(y_pred)\\n        results[\"Error_Rate\"].append(1 - cumulative_accuracy)\\n        results[\"Cumulative_Accuracy\"].append(cumulative_accuracy)\\n\\n\\n        X_walk = pd.concat([X_walk, x_i], ignore_index=True)\\n        y_walk = pd.concat([y_walk, pd.Series([y_true])], ignore_index=True)\\n\\n\\n        if (i + 1) % 5 == 0:\\n            X_walk_scaled = best_scaler.transform(X_walk)\\n            dynamic_model.fit(X_walk_scaled, y_walk)\\n\\n\\n    result_df = pd.DataFrame(results)\\n    print(f\"\\n{company_code} için walk-forward tamamlandı. Son doğruluk: {cumulative_accuracy:.2%}\")\\n    return result_df'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":51}]},{"cell_type":"code","source":["'''if __name__ == \"__main__\":\n","    result_dir = \"Results/Change/Beast\"\n","    os.makedirs(result_dir, exist_ok=True)\n","\n","    company_files = sorted(glob.glob(\"Datas/*.csv\"))\n","    print(f\"İşlenecek toplam şirket sayısı: {len(company_files)}\")\n","\n","    for idx, filepath in enumerate(company_files):\n","        company_code = os.path.splitext(os.path.basename(filepath))[0]\n","        print(f\"\\n[{idx+1}/{len(company_files)}] {company_code} işleniyor...\")\n","\n","        try:\n","            result_df = DRpredictor(company_code)\n","\n","            if not result_df.empty:\n","                result_path = os.path.join(result_dir, f\"{company_code}_results.csv\")\n","                result_df.to_csv(result_path, index=False)\n","                print(f\"→ Sonuçlar kaydedildi: {result_path}\")\n","        except Exception as e:\n","            print(f\"{company_code} işlenirken hata: {str(e)}\")'''"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":157},"id":"bLucHdiZ43gk","executionInfo":{"status":"ok","timestamp":1749838587868,"user_tz":-180,"elapsed":6,"user":{"displayName":"Osman Mango","userId":"07182088882667002738"}},"outputId":"5cebe206-a076-4078-f81c-b89a7fdcb37d"},"id":"bLucHdiZ43gk","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'if __name__ == \"__main__\":\\n    result_dir = \"Results/Change/Beast\"\\n    os.makedirs(result_dir, exist_ok=True)\\n\\n    company_files = sorted(glob.glob(\"Datas/*.csv\"))\\n    print(f\"İşlenecek toplam şirket sayısı: {len(company_files)}\")\\n\\n    for idx, filepath in enumerate(company_files):\\n        company_code = os.path.splitext(os.path.basename(filepath))[0]\\n        print(f\"\\n[{idx+1}/{len(company_files)}] {company_code} işleniyor...\")\\n\\n        try:\\n            result_df = DRpredictor(company_code)\\n\\n            if not result_df.empty:\\n                result_path = os.path.join(result_dir, f\"{company_code}_results.csv\")\\n                result_df.to_csv(result_path, index=False)\\n                print(f\"→ Sonuçlar kaydedildi: {result_path}\")\\n        except Exception as e:\\n            print(f\"{company_code} işlenirken hata: {str(e)}\")'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":52}]},{"cell_type":"code","source":["'''CONFIG = {\n","    \"window_size\": 64,\n","    \"test_size\": 64,\n","    \"data_paths\": [\"Datas/*.csv\", \"Test/*.csv\"],\n","    \"start_date\": \"2006-07-27\",\n","    \"end_date\": \"2025-05-06\",\n","    \"n_splits\": 12,\n","    \"feature_columns\": [\"Close\", \"High\", \"Low\", \"Open\", \"Volume\", \"MA_20\", \"RSI\", \"Volatility\"],\n","    \"target_columns\": [\"CDirection\", \"Change\"]\n","}\n","\n","def load_and_preprocess_single(file_path: str) -> pd.DataFrame:\n","    try:\n","        df = pd.read_csv(\n","            file_path,\n","            skiprows=2,\n","            header=None,\n","            names=[\"Date\", \"Close\", \"High\", \"Low\", \"Open\", \"Volume\"]\n","        )\n","\n","        # Tarih dönüşümü ve filtreleme\n","        df[\"Date\"] = pd.to_datetime(df[\"Date\"], errors='coerce', format='mixed')\n","        df = df.dropna(subset=[\"Date\"])\n","\n","        # Tarih aralığı filtresi\n","        mask = (\n","            (df[\"Date\"] >= pd.to_datetime(CONFIG[\"start_date\"])) &\n","            (df[\"Date\"] <= pd.to_datetime(CONFIG[\"end_date\"]))\n","        )\n","        df = df.loc[mask]\n","\n","        if df.empty:\n","            print(f\"Uyarı: {os.path.basename(file_path)} dosyası filtreleme sonrası boş\")\n","            return pd.DataFrame()\n","\n","        # Sayısal sütun dönüşümü\n","        numeric_cols = [\"Close\", \"High\", \"Low\", \"Open\", \"Volume\"]\n","        df[numeric_cols] = df[numeric_cols].apply(pd.to_numeric, errors='coerce')\n","        df = df.dropna(subset=numeric_cols)\n","\n","        return df\n","\n","    except Exception as e:\n","        print(f\"Hata: {file_path} yüklenirken - {str(e)}\")\n","        return pd.DataFrame()\n","\n","def calculate_features(df):\n","    df = df.copy()\n","    df[\"Change\"] = df[\"Close\"].pct_change(fill_method=None) * 100\n","    df[\"CDirection\"] = np.where(df[\"Change\"] < 0, 0, 1)\n","\n","\n","    df[\"MA_20\"] = df[\"Close\"].rolling(20, min_periods=1).mean()\n","\n","\n","    rsi = ta.momentum.RSIIndicator(df[\"Close\"], window=14).rsi()\n","    df[\"RSI\"] = rsi.fillna(50)\n","\n","\n","    volatility = df[\"Close\"].pct_change().rolling(5, min_periods=1).std()\n","    df[\"Volatility\"] = volatility.fillna(0.01)\n","\n","    return df.dropna()\n","\n","def load_company_data(company_code: str) -> Tuple[pd.DataFrame, pd.DataFrame]:\n","    # Dosya yollarını oluştur\n","    train_path = f\"Datas/{company_code}.csv\"\n","    test_path = f\"Test/{company_code}.csv\"\n","\n","    # Verileri yükle\n","    train_df = load_and_preprocess_single(train_path)\n","    test_df = load_and_preprocess_single(test_path)\n","\n","    if train_df.empty or test_df.empty:\n","        return pd.DataFrame(), pd.DataFrame()\n","\n","    # Özellik mühendisliği\n","    train_df = calculate_features(train_df)\n","    test_df = calculate_features(test_df)\n","\n","    return train_df, test_df\n","\n","def prepare_data(df_train: pd.DataFrame, df_test: pd.DataFrame) -> tuple:\n","    \"\"\"Veriyi hazırlama fonksiyonu (hem yön hem de değişim için)\"\"\"\n","    # Tarih özellikleri\n","    def add_date_features(df):\n","        dt = df[\"Date\"].dt\n","        return df.assign(\n","            Year=dt.year,\n","            Month=dt.month,\n","            Day=dt.day,\n","            Weekday=dt.weekday\n","        )\n","\n","    df_train = add_date_features(df_train)\n","    df_test = add_date_features(df_test)\n","\n","    # Özellikler ve hedefler\n","    feature_cols = [col for col in df_train.columns\n","                   if col not in [\"Date\", \"Change\", \"CDirection\"] and col in CONFIG[\"feature_columns\"]]\n","\n","    # Eğitim verisi\n","    X_train = df_train[feature_cols]\n","    y_train = df_train[CONFIG[\"target_columns\"]]\n","\n","    # Test verisi\n","    actual_values = df_test[CONFIG[\"target_columns\"]].reset_index(drop=True)\n","    test_dates = df_test[\"Date\"].reset_index(drop=True)\n","    X_test = df_test[feature_cols]\n","\n","    return X_train, y_train, actual_values, test_dates, X_test\n","\n","def DRpredictor(company_code: str) -> pd.DataFrame:\n","    # Veri yükleme ve hazırlama\n","    train_df, test_df = load_company_data(company_code)\n","    if train_df.empty or test_df.empty:\n","        return pd.DataFrame()\n","\n","    X_train, y_train, actual_values, test_dates, X_test = prepare_data(train_df, test_df)\n","\n","    # Scaler'lar\n","    feature_scaler = StandardScaler()\n","    change_scaler = MinMaxScaler(feature_range=(-1, 1))\n","\n","    # Veriyi ölçekle\n","    X_train_scaled = feature_scaler.fit_transform(X_train)\n","    y_change_scaled = change_scaler.fit_transform(y_train[[\"Change\"]])\n","\n","    # Çift hedef için veri hazırlama\n","    y_train_multi = np.column_stack((\n","        y_train[\"CDirection\"].values,\n","        y_change_scaled.flatten()\n","    ))\n","\n","    # Time Series Cross Validation\n","    tscv = TimeSeriesSplit(n_splits=CONFIG[\"n_splits\"])\n","    best_val_loss = float('inf')\n","    best_model = None\n","\n","    for fold, (train_idx, val_idx) in enumerate(tscv.split(X_train_scaled), 1):\n","        # Veriyi böl\n","        X_t, X_v = X_train_scaled[train_idx], X_train_scaled[val_idx]\n","        y_t, y_v = y_train_multi[train_idx], y_train_multi[val_idx]\n","\n","        # Model oluştur (çift çıkışlı)\n","        model = MultiOutputRegressor(\n","            SGDRegressor(\n","                loss='huber',\n","                learning_rate='invscaling',\n","                eta0=0.01,\n","                power_t=0.25,\n","                early_stopping=True,\n","                validation_fraction=0.2,\n","                n_iter_no_change=5,\n","                random_state=42\n","            )\n","        )\n","        model.fit(X_t, y_t)\n","\n","        # Validasyon hatası\n","        y_pred = model.predict(X_v)\n","        val_loss = mean_squared_error(y_v, y_pred)\n","\n","        # En iyi modeli güncelle\n","        if val_loss < best_val_loss:\n","            best_val_loss = val_loss\n","            best_model = deepcopy(model)\n","\n","    print(f\"En iyi model val_loss: {best_val_loss:.4f}\")\n","\n","    # Walk-forward test süreci\n","    results = {\n","        \"Date\": test_dates.tolist(),\n","        \"Real_CDirection\": actual_values[\"CDirection\"].tolist(),\n","        \"Predicted_CDirection\": [],\n","        \"Real_Change\": actual_values[\"Change\"].tolist(),\n","        \"Predicted_Change\": [],\n","        \"Direction_Accuracy\": [],\n","        \"Change_RMSE\": []\n","    }\n","\n","    # Online öğrenme için model\n","    online_model = deepcopy(best_model)\n","    X_walk = X_train.copy()\n","    y_walk = y_train.copy()\n","\n","    # Doğru yön sayacı\n","    correct_direction_count = 0\n","\n","    for i in range(len(X_test)):\n","        # Test örneğini ölçeklendir\n","        x_i = X_test.iloc[[i]]\n","        x_i_scaled = feature_scaler.transform(x_i)\n","\n","        # Tahmin yap\n","        y_pred_scaled = online_model.predict(x_i_scaled)[0]\n","\n","        # Çıktıları dönüştür\n","        direction_pred = 1 if y_pred_scaled[0] > 0.5 else 0\n","        change_pred = change_scaler.inverse_transform([[y_pred_scaled[1]]])[0][0]\n","\n","        # Gerçek değerler\n","        direction_true = actual_values[\"CDirection\"].iloc[i]\n","        change_true = actual_values[\"Change\"].iloc[i]\n","\n","        # Yön doğruluğu\n","        correct_direction = direction_pred == direction_true\n","        correct_direction_count = correct_direction_count + 1 if correct_direction else correct_direction_count\n","        direction_accuracy = correct_direction_count / (i + 1)\n","\n","        # Değişim hatası (RMSE)\n","        current_rmse = np.sqrt(mean_squared_error(\n","            actual_values[\"Change\"].iloc[:i+1],\n","            results[\"Predicted_Change\"] + [change_pred]\n","        )) if i > 0 else 0\n","\n","        # Sonuçları kaydet\n","        results[\"Predicted_CDirection\"].append(direction_pred)\n","        results[\"Predicted_Change\"].append(change_pred)\n","        results[\"Direction_Accuracy\"].append(direction_accuracy)\n","        results[\"Change_RMSE\"].append(current_rmse)\n","\n","        # Modeli güncelle (online learning)\n","        X_walk = pd.concat([X_walk, x_i], ignore_index=True)\n","        y_walk = pd.concat([\n","            y_walk,\n","            pd.DataFrame([[direction_true, change_true]], columns=CONFIG[\"target_columns\"])\n","        ], ignore_index=True)\n","\n","        # Her 5 adımda bir modeli yeniden eğit\n","        if (i + 1) % 5 == 0:\n","            X_walk_scaled = feature_scaler.transform(X_walk)\n","            y_change_scaled = change_scaler.transform(y_walk[[\"Change\"]])\n","            y_walk_multi = np.column_stack((\n","                y_walk[\"CDirection\"].values,\n","                y_change_scaled.flatten()\n","            ))\n","            online_model.fit(X_walk_scaled, y_walk_multi)\n","\n","    # Sonuçları DataFrame'e dönüştür\n","    result_df = pd.DataFrame(results)\n","    final_acc = accuracy_score(\n","        result_df[\"Real_CDirection\"],\n","        result_df[\"Predicted_CDirection\"]\n","    )\n","    final_rmse = np.sqrt(mean_squared_error(\n","        result_df[\"Real_Change\"],\n","        result_df[\"Predicted_Change\"]\n","    ))\n","\n","    print(f\"{company_code} sonuçları: Yön Doğruluğu: {final_acc:.2%}, Change RMSE: {final_rmse:.4f}\")\n","    return result_df'''"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":192},"id":"YnZkiyJuSoBY","executionInfo":{"status":"ok","timestamp":1749993416478,"user_tz":-180,"elapsed":71,"user":{"displayName":"Osman Mango","userId":"07182088882667002738"}},"outputId":"adc332cf-59ce-4c79-cb89-19e20cd2d981"},"id":"YnZkiyJuSoBY","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'CONFIG = {\\n    \"window_size\": 64,\\n    \"test_size\": 64,\\n    \"data_paths\": [\"Datas/*.csv\", \"Test/*.csv\"],\\n    \"start_date\": \"2006-07-27\",\\n    \"end_date\": \"2025-05-06\",\\n    \"n_splits\": 12,\\n    \"feature_columns\": [\"Close\", \"High\", \"Low\", \"Open\", \"Volume\", \"MA_20\", \"RSI\", \"Volatility\"],\\n    \"target_columns\": [\"CDirection\", \"Change\"]\\n}\\n\\ndef load_and_preprocess_single(file_path: str) -> pd.DataFrame:\\n    try:\\n        df = pd.read_csv(\\n            file_path,\\n            skiprows=2,\\n            header=None,\\n            names=[\"Date\", \"Close\", \"High\", \"Low\", \"Open\", \"Volume\"]\\n        )\\n\\n        # Tarih dönüşümü ve filtreleme\\n        df[\"Date\"] = pd.to_datetime(df[\"Date\"], errors=\\'coerce\\', format=\\'mixed\\')\\n        df = df.dropna(subset=[\"Date\"])\\n\\n        # Tarih aralığı filtresi\\n        mask = (\\n            (df[\"Date\"] >= pd.to_datetime(CONFIG[\"start_date\"])) &\\n            (df[\"Date\"] <= pd.to_datetime(CONFIG[\"end_date\"]))\\n        )\\n        df = df.loc[mask]\\n\\n        if df.empty:\\n            print(f\"Uyarı: {os.path.basename(file_path)} dosyası filtreleme sonrası boş\")\\n            return pd.DataFrame()\\n\\n        # Sayısal sütun dönüşümü\\n        numeric_cols = [\"Close\", \"High\", \"Low\", \"Open\", \"Volume\"]\\n        df[numeric_cols] = df[numeric_cols].apply(pd.to_numeric, errors=\\'coerce\\')\\n        df = df.dropna(subset=numeric_cols)\\n\\n        return df\\n\\n    except Exception as e:\\n        print(f\"Hata: {file_path} yüklenirken - {str(e)}\")\\n        return pd.DataFrame()\\n\\ndef calculate_features(df):\\n    df = df.copy()\\n    df[\"Change\"] = df[\"Close\"].pct_change(fill_method=None) * 100\\n    df[\"CDirection\"] = np.where(df[\"Change\"] < 0, 0, 1)\\n\\n\\n    df[\"MA_20\"] = df[\"Close\"].rolling(20, min_periods=1).mean()\\n\\n\\n    rsi = ta.momentum.RSIIndicator(df[\"Close\"], window=14).rsi()\\n    df[\"RSI\"] = rsi.fillna(50)\\n\\n\\n    volatility = df[\"Close\"].pct_change().rolling(5, min_periods=1).std()\\n    df[\"Volatility\"] = volatility.fillna(0.01)\\n\\n    return df.dropna()\\n\\ndef load_company_data(company_code: str) -> Tuple[pd.DataFrame, pd.DataFrame]:\\n    # Dosya yollarını oluştur\\n    train_path = f\"Datas/{company_code}.csv\"\\n    test_path = f\"Test/{company_code}.csv\"\\n\\n    # Verileri yükle\\n    train_df = load_and_preprocess_single(train_path)\\n    test_df = load_and_preprocess_single(test_path)\\n\\n    if train_df.empty or test_df.empty:\\n        return pd.DataFrame(), pd.DataFrame()\\n\\n    # Özellik mühendisliği\\n    train_df = calculate_features(train_df)\\n    test_df = calculate_features(test_df)\\n\\n    return train_df, test_df\\n\\ndef prepare_data(df_train: pd.DataFrame, df_test: pd.DataFrame) -> tuple:\\n    \"\"\"Veriyi hazırlama fonksiyonu (hem yön hem de değişim için)\"\"\"\\n    # Tarih özellikleri\\n    def add_date_features(df):\\n        dt = df[\"Date\"].dt\\n        return df.assign(\\n            Year=dt.year,\\n            Month=dt.month,\\n            Day=dt.day,\\n            Weekday=dt.weekday\\n        )\\n\\n    df_train = add_date_features(df_train)\\n    df_test = add_date_features(df_test)\\n\\n    # Özellikler ve hedefler\\n    feature_cols = [col for col in df_train.columns\\n                   if col not in [\"Date\", \"Change\", \"CDirection\"] and col in CONFIG[\"feature_columns\"]]\\n\\n    # Eğitim verisi\\n    X_train = df_train[feature_cols]\\n    y_train = df_train[CONFIG[\"target_columns\"]]\\n\\n    # Test verisi\\n    actual_values = df_test[CONFIG[\"target_columns\"]].reset_index(drop=True)\\n    test_dates = df_test[\"Date\"].reset_index(drop=True)\\n    X_test = df_test[feature_cols]\\n\\n    return X_train, y_train, actual_values, test_dates, X_test\\n\\ndef DRpredictor(company_code: str) -> pd.DataFrame:\\n    # Veri yükleme ve hazırlama\\n    train_df, test_df = load_company_data(company_code)\\n    if train_df.empty or test_df.empty:\\n        return pd.DataFrame()\\n\\n    X_train, y_train, actual_values, test_dates, X_test = prepare_data(train_df, test_df)\\n\\n    # Scaler\\'lar\\n    feature_scaler = StandardScaler()\\n    change_scaler = MinMaxScaler(feature_range=(-1, 1))\\n\\n    # Veriyi ölçekle\\n    X_train_scaled = feature_scaler.fit_transform(X_train)\\n    y_change_scaled = change_scaler.fit_transform(y_train[[\"Change\"]])\\n\\n    # Çift hedef için veri hazırlama\\n    y_train_multi = np.column_stack((\\n        y_train[\"CDirection\"].values,\\n        y_change_scaled.flatten()\\n    ))\\n\\n    # Time Series Cross Validation\\n    tscv = TimeSeriesSplit(n_splits=CONFIG[\"n_splits\"])\\n    best_val_loss = float(\\'inf\\')\\n    best_model = None\\n\\n    for fold, (train_idx, val_idx) in enumerate(tscv.split(X_train_scaled), 1):\\n        # Veriyi böl\\n        X_t, X_v = X_train_scaled[train_idx], X_train_scaled[val_idx]\\n        y_t, y_v = y_train_multi[train_idx], y_train_multi[val_idx]\\n\\n        # Model oluştur (çift çıkışlı)\\n        model = MultiOutputRegressor(\\n            SGDRegressor(\\n                loss=\\'huber\\',\\n                learning_rate=\\'invscaling\\',\\n                eta0=0.01,\\n                power_t=0.25,\\n                early_stopping=True,\\n                validation_fraction=0.2,\\n                n_iter_no_change=5,\\n                random_state=42\\n            )\\n        )\\n        model.fit(X_t, y_t)\\n\\n        # Validasyon hatası\\n        y_pred = model.predict(X_v)\\n        val_loss = mean_squared_error(y_v, y_pred)\\n\\n        # En iyi modeli güncelle\\n        if val_loss < best_val_loss:\\n            best_val_loss = val_loss\\n            best_model = deepcopy(model)\\n\\n    print(f\"En iyi model val_loss: {best_val_loss:.4f}\")\\n\\n    # Walk-forward test süreci\\n    results = {\\n        \"Date\": test_dates.tolist(),\\n        \"Real_CDirection\": actual_values[\"CDirection\"].tolist(),\\n        \"Predicted_CDirection\": [],\\n        \"Real_Change\": actual_values[\"Change\"].tolist(),\\n        \"Predicted_Change\": [],\\n        \"Direction_Accuracy\": [],\\n        \"Change_RMSE\": []\\n    }\\n\\n    # Online öğrenme için model\\n    online_model = deepcopy(best_model)\\n    X_walk = X_train.copy()\\n    y_walk = y_train.copy()\\n\\n    # Doğru yön sayacı\\n    correct_direction_count = 0\\n\\n    for i in range(len(X_test)):\\n        # Test örneğini ölçeklendir\\n        x_i = X_test.iloc[[i]]\\n        x_i_scaled = feature_scaler.transform(x_i)\\n\\n        # Tahmin yap\\n        y_pred_scaled = online_model.predict(x_i_scaled)[0]\\n\\n        # Çıktıları dönüştür\\n        direction_pred = 1 if y_pred_scaled[0] > 0.5 else 0\\n        change_pred = change_scaler.inverse_transform([[y_pred_scaled[1]]])[0][0]\\n\\n        # Gerçek değerler\\n        direction_true = actual_values[\"CDirection\"].iloc[i]\\n        change_true = actual_values[\"Change\"].iloc[i]\\n\\n        # Yön doğruluğu\\n        correct_direction = direction_pred == direction_true\\n        correct_direction_count = correct_direction_count + 1 if correct_direction else correct_direction_count\\n        direction_accuracy = correct_direction_count / (i + 1)\\n\\n        # Değişim hatası (RMSE)\\n        current_rmse = np.sqrt(mean_squared_error(\\n            actual_values[\"Change\"].iloc[:i+1],\\n            results[\"Predicted_Change\"] + [change_pred]\\n        )) if i > 0 else 0\\n\\n        # Sonuçları kaydet\\n        results[\"Predicted_CDirection\"].append(direction_pred)\\n        results[\"Predicted_Change\"].append(change_pred)\\n        results[\"Direction_Accuracy\"].append(direction_accuracy)\\n        results[\"Change_RMSE\"].append(current_rmse)\\n\\n        # Modeli güncelle (online learning)\\n        X_walk = pd.concat([X_walk, x_i], ignore_index=True)\\n        y_walk = pd.concat([\\n            y_walk,\\n            pd.DataFrame([[direction_true, change_true]], columns=CONFIG[\"target_columns\"])\\n        ], ignore_index=True)\\n\\n        # Her 5 adımda bir modeli yeniden eğit\\n        if (i + 1) % 5 == 0:\\n            X_walk_scaled = feature_scaler.transform(X_walk)\\n            y_change_scaled = change_scaler.transform(y_walk[[\"Change\"]])\\n            y_walk_multi = np.column_stack((\\n                y_walk[\"CDirection\"].values,\\n                y_change_scaled.flatten()\\n            ))\\n            online_model.fit(X_walk_scaled, y_walk_multi)\\n\\n    # Sonuçları DataFrame\\'e dönüştür\\n    result_df = pd.DataFrame(results)\\n    final_acc = accuracy_score(\\n        result_df[\"Real_CDirection\"],\\n        result_df[\"Predicted_CDirection\"]\\n    )\\n    final_rmse = np.sqrt(mean_squared_error(\\n        result_df[\"Real_Change\"],\\n        result_df[\"Predicted_Change\"]\\n    ))\\n\\n    print(f\"{company_code} sonuçları: Yön Doğruluğu: {final_acc:.2%}, Change RMSE: {final_rmse:.4f}\")\\n    return result_df'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":["'''# Ana işlem\n","if __name__ == \"__main__\":\n","    result_dir = \"Results/Change/Beast\"\n","    os.makedirs(result_dir, exist_ok=True)\n","\n","    company_files = sorted(glob.glob(\"Datas/*.csv\"))\n","    print(f\"İşlenecek toplam şirket sayısı: {len(company_files)}\")\n","\n","    for idx, filepath in enumerate(company_files):\n","        company_code = os.path.splitext(os.path.basename(filepath))[0]\n","        print(f\"\\n[{idx+1}/{len(company_files)}] {company_code} işleniyor...\")\n","\n","        try:\n","            result_df = DRpredictor(company_code)\n","            if not result_df.empty:\n","                result_path = os.path.join(result_dir, f\"{company_code}_results.csv\")\n","                result_df.to_csv(result_path, index=False)\n","                print(f\"→ Sonuçlar kaydedildi: {result_path}\")\n","        except Exception as e:\n","            print(f\"{company_code} işlenirken hata: {str(e)}\")'''"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":157},"id":"gpJcIYCSSvs_","executionInfo":{"status":"ok","timestamp":1749993422458,"user_tz":-180,"elapsed":27,"user":{"displayName":"Osman Mango","userId":"07182088882667002738"}},"outputId":"25a37998-4def-49b9-921e-3c3d708db41c"},"id":"gpJcIYCSSvs_","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'# Ana işlem\\nif __name__ == \"__main__\":\\n    result_dir = \"Results/Change/Beast\"\\n    os.makedirs(result_dir, exist_ok=True)\\n\\n    company_files = sorted(glob.glob(\"Datas/*.csv\"))\\n    print(f\"İşlenecek toplam şirket sayısı: {len(company_files)}\")\\n\\n    for idx, filepath in enumerate(company_files):\\n        company_code = os.path.splitext(os.path.basename(filepath))[0]\\n        print(f\"\\n[{idx+1}/{len(company_files)}] {company_code} işleniyor...\")\\n\\n        try:\\n            result_df = DRpredictor(company_code)\\n            if not result_df.empty:\\n                result_path = os.path.join(result_dir, f\"{company_code}_results.csv\")\\n                result_df.to_csv(result_path, index=False)\\n                print(f\"→ Sonuçlar kaydedildi: {result_path}\")\\n        except Exception as e:\\n            print(f\"{company_code} işlenirken hata: {str(e)}\")'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":21}]},{"cell_type":"code","source":["'''CONFIG = {\n","    \"window_size\": 64,\n","    \"test_size\": 64,\n","    \"data_paths\": [\"Datas/*.csv\", \"Test/*.csv\"],\n","    \"start_date\": \"2006-07-27\",\n","    \"end_date\": \"2025-05-06\",\n","    \"n_splits\": 12,\n","}\n","\n","def load_and_preprocess_single(file_path: str) -> pd.DataFrame:\n","    try:\n","        df = pd.read_csv(\n","            file_path,\n","            skiprows=2,\n","            header=None,\n","            names=[\"Date\", \"Close\", \"High\", \"Low\", \"Open\", \"Volume\"]\n","        )\n","\n","        df[\"Date\"] = pd.to_datetime(df[\"Date\"], errors='coerce', format='mixed')\n","        df = df.dropna(subset=[\"Date\"])\n","\n","        end_date = pd.to_datetime(CONFIG[\"end_date\"]) + timedelta(days=1)\n","        mask = (df[\"Date\"] >= pd.to_datetime(CONFIG[\"start_date\"])) & (df[\"Date\"] <= end_date)\n","        df = df.loc[mask]\n","\n","        if df.empty:\n","            print(f\"Uyarı: {os.path.basename(file_path)} dosyası filtreleme sonrası boş\")\n","            return pd.DataFrame()\n","\n","        for col in [\"Close\", \"High\", \"Low\", \"Open\", \"Volume\"]:\n","            df[col] = pd.to_numeric(df[col], errors='coerce')\n","\n","        return df.dropna()\n","\n","    except Exception as e:\n","        print(f\"Hata: {file_path} yüklenirken - {str(e)}\")\n","        return pd.DataFrame()\n","\n","def load_company_data(company_code: str) -> Tuple[pd.DataFrame, pd.DataFrame]:\n","    train_path = f\"Datas/{company_code}.csv\"\n","    test_path = f\"Test/{company_code}.csv\"\n","\n","    train_df = load_and_preprocess_single(train_path)\n","    test_df = load_and_preprocess_single(test_path)\n","\n","    if train_df.empty or test_df.empty:\n","        return pd.DataFrame(), pd.DataFrame()\n","\n","    def calculate_features(df):\n","        df = df.copy()\n","        # Önceki güne göre yüzde değişim\n","        df[\"Change\"] = df[\"Close\"].pct_change(fill_method=None) * 100\n","\n","        # Özellik mühendisliği\n","        df[\"MA_20\"] = df[\"Close\"].rolling(20, min_periods=1).mean()\n","\n","        # RSI hesaplama\n","        rsi = ta.momentum.RSIIndicator(df[\"Close\"], window=14).rsi()\n","        df[\"RSI\"] = rsi.fillna(50)\n","\n","        # Volatilite\n","        volatility = df[\"Close\"].pct_change().rolling(5, min_periods=1).std()\n","        df[\"Volatility\"] = volatility.fillna(0.01)\n","\n","        # Önceki günün kapanışı\n","        df[\"Prev_Close\"] = df[\"Close\"].shift(1)\n","\n","        return df.dropna()\n","\n","    train_df = calculate_features(train_df)\n","    test_df = calculate_features(test_df)\n","\n","    return train_df, test_df\n","\n","def dater(df_train: pd.DataFrame, df_test: pd.DataFrame) -> tuple:\n","    def process_date_features(df):\n","        df = df.copy()\n","        df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n","        df[\"Year\"] = df[\"Date\"].dt.year\n","        df[\"Month\"] = df[\"Date\"].dt.month\n","        df[\"Day\"] = df[\"Date\"].dt.day\n","        df[\"Weekday\"] = df[\"Date\"].dt.weekday\n","        return df\n","\n","    df_train = process_date_features(df_train)\n","    df_test = process_date_features(df_test)\n","\n","    # Hedef değişken ve özellikler\n","    y_train = df_train[\"Change\"]\n","    # Gereksiz sütunları kaldır\n","    X_train = df_train.drop([\"Date\", \"Change\", \"CDirection\"], axis=1, errors=\"ignore\")\n","\n","    actual_change = df_test[\"Change\"].reset_index(drop=True)\n","    test_dates = df_test[\"Date\"].reset_index(drop=True)\n","    X_test = df_test.drop([\"Date\", \"Change\", \"CDirection\"], axis=1, errors=\"ignore\")\n","\n","    return X_train, y_train, actual_change, test_dates, X_test\n","\n","def DRpredictor(company_code: str) -> pd.DataFrame:\n","    train_df, test_df = load_company_data(company_code)\n","\n","    if train_df.empty or test_df.empty:\n","        print(f\"{company_code} için yeterli veri yok. Atlanıyor.\")\n","        return pd.DataFrame()\n","\n","    X_train, y_train, actual_change, test_dates, X_test = dater(train_df, test_df)\n","\n","    # Hacim sütununu log dönüşümü\n","    for df in [X_train, X_test]:\n","        if \"Volume\" in df.columns:\n","            df[\"Volume\"] = np.log1p(df[\"Volume\"])\n","\n","    tscv = TimeSeriesSplit(n_splits=CONFIG[\"n_splits\"])\n","    best_val_loss = float('inf')\n","    best_scaler = None\n","    best_model = None\n","\n","    for fold, (train_idx, val_idx) in enumerate(tscv.split(X_train), 1):\n","        X_t, X_v = X_train.iloc[train_idx], X_train.iloc[val_idx]\n","        y_t, y_v = y_train.iloc[train_idx], y_train.iloc[val_idx]\n","\n","        scaler = MinMaxScaler(feature_range=(-1, 1))\n","        X_t_scaled = scaler.fit_transform(X_t)\n","        X_v_scaled = scaler.transform(X_v)\n","\n","        model = SGDRegressor(\n","            alpha=0.0001,  # Regularizasyon\n","            learning_rate='invscaling',\n","            eta0=0.001,  # Düşük öğrenme oranı\n","            max_iter=1000,\n","            tol=1e-3,\n","            random_state=42,\n","            early_stopping=False,\n","            shuffle=False\n","        )\n","        model.fit(X_t_scaled, y_t)\n","\n","        y_pred_val = model.predict(X_v_scaled)\n","        val_loss = mean_squared_error(y_v, y_pred_val)\n","\n","        if val_loss < best_val_loss:\n","            best_val_loss = val_loss\n","            best_scaler = scaler\n","            best_model = model\n","\n","    print(f\"\\nEn iyi model val_loss (MSE): {best_val_loss:.6f}\")\n","\n","    # Sonuçları saklamak için yapı\n","    results = {\n","        \"Date\": [],\n","        \"Real_Change\": [],\n","        \"Predicted_Change\": [],\n","        \"Price_Error\": [],\n","        \"Squared_Error\": [],\n","        \"Cumulative_MSE\": []\n","    }\n","\n","    total_squared_error = 0\n","    X_walk = X_train.copy()\n","    y_walk = y_train.copy()\n","\n","    dynamic_model = HuberRegressor(\n","        alpha=0.0001,\n","        max_iter=1000,\n","        warm_start=True\n","    )\n","\n","    # İlk modeli eğit\n","    X_walk_scaled = best_scaler.transform(X_walk)\n","    dynamic_model.fit(X_walk_scaled, y_walk)\n","\n","    for i in range(20):\n","        x_i = X_test.iloc[[i]]\n","        x_i_scaled = best_scaler.transform(x_i)\n","\n","        y_pred = best_model.predict(x_i_scaled)[0]\n","        y_true = actual_change.iloc[i]\n","        current_date = test_dates.iloc[i]\n","\n","        price_error = y_true - y_pred\n","        squared_error = (y_true - y_pred) ** 2\n","        total_squared_error += squared_error\n","        cumulative_mse = total_squared_error / (i + 1)\n","\n","        results[\"Date\"].append(current_date)\n","        results[\"Real_Change\"].append(y_true)\n","        results[\"Predicted_Change\"].append(y_pred)\n","        results[\"Price_Error\"].append(price_error)\n","        results[\"Squared_Error\"].append(squared_error)\n","        results[\"Cumulative_MSE\"].append(cumulative_mse)\n","\n","        # Modeli güncelle\n","        X_walk = pd.concat([X_walk, x_i], ignore_index=True)\n","        y_walk = pd.concat([y_walk, pd.Series([y_true])], ignore_index=True)\n","\n","        # Her 5 adımda bir modeli yeniden eğit\n","        if (i + 1) % 5 == 0:\n","            X_walk_scaled = best_scaler.transform(X_walk)\n","            best_model.fit(X_walk_scaled, y_walk)\n","\n","    result_df = pd.DataFrame(results)\n","    print(f\"\\n{company_code} için walk-forward tamamlandı. Son MSE: {cumulative_mse:.6f}\")\n","    return result_df'''"],"metadata":{"id":"lHUEHloOZJc0","executionInfo":{"status":"ok","timestamp":1749994115042,"user_tz":-180,"elapsed":51,"user":{"displayName":"Osman Mango","userId":"07182088882667002738"}},"colab":{"base_uri":"https://localhost:8080/","height":192},"outputId":"fff03b27-55e6-4f17-b8e4-00260bd9e0f1"},"id":"lHUEHloOZJc0","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'CONFIG = {\\n    \"window_size\": 64,\\n    \"test_size\": 64,\\n    \"data_paths\": [\"Datas/*.csv\", \"Test/*.csv\"],\\n    \"start_date\": \"2006-07-27\",\\n    \"end_date\": \"2025-05-06\",\\n    \"n_splits\": 12,\\n}\\n\\ndef load_and_preprocess_single(file_path: str) -> pd.DataFrame:\\n    try:\\n        df = pd.read_csv(\\n            file_path,\\n            skiprows=2,\\n            header=None,\\n            names=[\"Date\", \"Close\", \"High\", \"Low\", \"Open\", \"Volume\"]\\n        )\\n\\n        df[\"Date\"] = pd.to_datetime(df[\"Date\"], errors=\\'coerce\\', format=\\'mixed\\')\\n        df = df.dropna(subset=[\"Date\"])\\n\\n        end_date = pd.to_datetime(CONFIG[\"end_date\"]) + timedelta(days=1)\\n        mask = (df[\"Date\"] >= pd.to_datetime(CONFIG[\"start_date\"])) & (df[\"Date\"] <= end_date)\\n        df = df.loc[mask]\\n\\n        if df.empty:\\n            print(f\"Uyarı: {os.path.basename(file_path)} dosyası filtreleme sonrası boş\")\\n            return pd.DataFrame()\\n\\n        for col in [\"Close\", \"High\", \"Low\", \"Open\", \"Volume\"]:\\n            df[col] = pd.to_numeric(df[col], errors=\\'coerce\\')\\n\\n        return df.dropna()\\n\\n    except Exception as e:\\n        print(f\"Hata: {file_path} yüklenirken - {str(e)}\")\\n        return pd.DataFrame()\\n\\ndef load_company_data(company_code: str) -> Tuple[pd.DataFrame, pd.DataFrame]:\\n    train_path = f\"Datas/{company_code}.csv\"\\n    test_path = f\"Test/{company_code}.csv\"\\n\\n    train_df = load_and_preprocess_single(train_path)\\n    test_df = load_and_preprocess_single(test_path)\\n\\n    if train_df.empty or test_df.empty:\\n        return pd.DataFrame(), pd.DataFrame()\\n\\n    def calculate_features(df):\\n        df = df.copy()\\n        # Önceki güne göre yüzde değişim\\n        df[\"Change\"] = df[\"Close\"].pct_change(fill_method=None) * 100\\n\\n        # Özellik mühendisliği\\n        df[\"MA_20\"] = df[\"Close\"].rolling(20, min_periods=1).mean()\\n\\n        # RSI hesaplama\\n        rsi = ta.momentum.RSIIndicator(df[\"Close\"], window=14).rsi()\\n        df[\"RSI\"] = rsi.fillna(50)\\n\\n        # Volatilite\\n        volatility = df[\"Close\"].pct_change().rolling(5, min_periods=1).std()\\n        df[\"Volatility\"] = volatility.fillna(0.01)\\n\\n        # Önceki günün kapanışı\\n        df[\"Prev_Close\"] = df[\"Close\"].shift(1)\\n\\n        return df.dropna()\\n\\n    train_df = calculate_features(train_df)\\n    test_df = calculate_features(test_df)\\n\\n    return train_df, test_df\\n\\ndef dater(df_train: pd.DataFrame, df_test: pd.DataFrame) -> tuple:\\n    def process_date_features(df):\\n        df = df.copy()\\n        df[\"Date\"] = pd.to_datetime(df[\"Date\"])\\n        df[\"Year\"] = df[\"Date\"].dt.year\\n        df[\"Month\"] = df[\"Date\"].dt.month\\n        df[\"Day\"] = df[\"Date\"].dt.day\\n        df[\"Weekday\"] = df[\"Date\"].dt.weekday\\n        return df\\n\\n    df_train = process_date_features(df_train)\\n    df_test = process_date_features(df_test)\\n\\n    # Hedef değişken ve özellikler\\n    y_train = df_train[\"Change\"]\\n    # Gereksiz sütunları kaldır\\n    X_train = df_train.drop([\"Date\", \"Change\", \"CDirection\"], axis=1, errors=\"ignore\")\\n\\n    actual_change = df_test[\"Change\"].reset_index(drop=True)\\n    test_dates = df_test[\"Date\"].reset_index(drop=True)\\n    X_test = df_test.drop([\"Date\", \"Change\", \"CDirection\"], axis=1, errors=\"ignore\")\\n\\n    return X_train, y_train, actual_change, test_dates, X_test\\n\\ndef DRpredictor(company_code: str) -> pd.DataFrame:\\n    train_df, test_df = load_company_data(company_code)\\n\\n    if train_df.empty or test_df.empty:\\n        print(f\"{company_code} için yeterli veri yok. Atlanıyor.\")\\n        return pd.DataFrame()\\n\\n    X_train, y_train, actual_change, test_dates, X_test = dater(train_df, test_df)\\n\\n    # Hacim sütununu log dönüşümü\\n    for df in [X_train, X_test]:\\n        if \"Volume\" in df.columns:\\n            df[\"Volume\"] = np.log1p(df[\"Volume\"])\\n\\n    tscv = TimeSeriesSplit(n_splits=CONFIG[\"n_splits\"])\\n    best_val_loss = float(\\'inf\\')\\n    best_scaler = None\\n    best_model = None\\n\\n    for fold, (train_idx, val_idx) in enumerate(tscv.split(X_train), 1):\\n        X_t, X_v = X_train.iloc[train_idx], X_train.iloc[val_idx]\\n        y_t, y_v = y_train.iloc[train_idx], y_train.iloc[val_idx]\\n\\n        scaler = MinMaxScaler(feature_range=(-1, 1))\\n        X_t_scaled = scaler.fit_transform(X_t)\\n        X_v_scaled = scaler.transform(X_v)\\n\\n        model = SGDRegressor(\\n            alpha=0.0001,  # Regularizasyon\\n            learning_rate=\\'invscaling\\',\\n            eta0=0.001,  # Düşük öğrenme oranı\\n            max_iter=1000,\\n            tol=1e-3,\\n            random_state=42,\\n            early_stopping=False,\\n            shuffle=False\\n        )\\n        model.fit(X_t_scaled, y_t)\\n\\n        y_pred_val = model.predict(X_v_scaled)\\n        val_loss = mean_squared_error(y_v, y_pred_val)\\n\\n        if val_loss < best_val_loss:\\n            best_val_loss = val_loss\\n            best_scaler = scaler\\n            best_model = model\\n\\n    print(f\"\\nEn iyi model val_loss (MSE): {best_val_loss:.6f}\")\\n\\n    # Sonuçları saklamak için yapı\\n    results = {\\n        \"Date\": [],\\n        \"Real_Change\": [],\\n        \"Predicted_Change\": [],\\n        \"Price_Error\": [],\\n        \"Squared_Error\": [],\\n        \"Cumulative_MSE\": []\\n    }\\n\\n    total_squared_error = 0\\n    X_walk = X_train.copy()\\n    y_walk = y_train.copy()\\n\\n    dynamic_model = HuberRegressor(\\n        alpha=0.0001,\\n        max_iter=1000,\\n        warm_start=True\\n    )\\n\\n    # İlk modeli eğit\\n    X_walk_scaled = best_scaler.transform(X_walk)\\n    dynamic_model.fit(X_walk_scaled, y_walk)\\n\\n    for i in range(20):\\n        x_i = X_test.iloc[[i]]\\n        x_i_scaled = best_scaler.transform(x_i)\\n\\n        y_pred = best_model.predict(x_i_scaled)[0]\\n        y_true = actual_change.iloc[i]\\n        current_date = test_dates.iloc[i]\\n\\n        price_error = y_true - y_pred\\n        squared_error = (y_true - y_pred) ** 2\\n        total_squared_error += squared_error\\n        cumulative_mse = total_squared_error / (i + 1)\\n\\n        results[\"Date\"].append(current_date)\\n        results[\"Real_Change\"].append(y_true)\\n        results[\"Predicted_Change\"].append(y_pred)\\n        results[\"Price_Error\"].append(price_error)\\n        results[\"Squared_Error\"].append(squared_error)\\n        results[\"Cumulative_MSE\"].append(cumulative_mse)\\n\\n        # Modeli güncelle\\n        X_walk = pd.concat([X_walk, x_i], ignore_index=True)\\n        y_walk = pd.concat([y_walk, pd.Series([y_true])], ignore_index=True)\\n\\n        # Her 5 adımda bir modeli yeniden eğit\\n        if (i + 1) % 5 == 0:\\n            X_walk_scaled = best_scaler.transform(X_walk)\\n            best_model.fit(X_walk_scaled, y_walk)\\n\\n    result_df = pd.DataFrame(results)\\n    print(f\"\\n{company_code} için walk-forward tamamlandı. Son MSE: {cumulative_mse:.6f}\")\\n    return result_df'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":26}]},{"cell_type":"code","source":["'''if __name__ == \"__main__\":\n","    result_dir = \"Results/Change/Beast\"\n","    os.makedirs(result_dir, exist_ok=True)\n","\n","    company_files = sorted(glob.glob(\"Datas/*.csv\"))\n","    print(f\"İşlenecek toplam şirket sayısı: {len(company_files)}\")\n","\n","    for idx, filepath in enumerate(company_files):\n","        company_code = os.path.splitext(os.path.basename(filepath))[0]\n","        print(f\"\\n[{idx+1}/{len(company_files)}] {company_code} işleniyor...\")\n","\n","        try:\n","            result_df = DRpredictor(company_code)\n","\n","            if not result_df.empty:\n","                result_path = os.path.join(result_dir, f\"{company_code}_results.csv\")\n","                result_df.to_csv(result_path, index=False)\n","                print(f\"→ Sonuçlar kaydedildi: {result_path}\")\n","        except Exception as e:\n","            print(f\"{company_code} işlenirken hata: {str(e)}\")'''"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":157},"id":"oMejHwg3ZWLW","executionInfo":{"status":"ok","timestamp":1749994102322,"user_tz":-180,"elapsed":6,"user":{"displayName":"Osman Mango","userId":"07182088882667002738"}},"outputId":"b58ac4f1-64ed-4cea-f715-8ae4ad765fb1"},"id":"oMejHwg3ZWLW","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'if __name__ == \"__main__\":\\n    result_dir = \"Results/Change/Beast\"\\n    os.makedirs(result_dir, exist_ok=True)\\n\\n    company_files = sorted(glob.glob(\"Datas/*.csv\"))\\n    print(f\"İşlenecek toplam şirket sayısı: {len(company_files)}\")\\n\\n    for idx, filepath in enumerate(company_files):\\n        company_code = os.path.splitext(os.path.basename(filepath))[0]\\n        print(f\"\\n[{idx+1}/{len(company_files)}] {company_code} işleniyor...\")\\n\\n        try:\\n            result_df = DRpredictor(company_code)\\n\\n            if not result_df.empty:\\n                result_path = os.path.join(result_dir, f\"{company_code}_results.csv\")\\n                result_df.to_csv(result_path, index=False)\\n                print(f\"→ Sonuçlar kaydedildi: {result_path}\")\\n        except Exception as e:\\n            print(f\"{company_code} işlenirken hata: {str(e)}\")'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":25}]},{"cell_type":"code","source":["'''CONFIG = {\n","    \"window_size\": 64,\n","    \"test_size\": 64,\n","    \"data_paths\": [\"Datas/*.csv\", \"Test/*.csv\"],\n","    \"start_date\": \"2006-07-27\",\n","    \"end_date\": \"2025-05-06\",\n","    \"n_splits\": 15,\n","}\n","\n","def load_and_preprocess_single(file_path: str) -> pd.DataFrame:\n","    try:\n","        df = pd.read_csv(\n","            file_path,\n","            skiprows=2,\n","            header=None,\n","            names=[\"Date\", \"Close\", \"High\", \"Low\", \"Open\", \"Volume\"]\n","        )\n","\n","        df[\"Date\"] = pd.to_datetime(df[\"Date\"], errors='coerce', format=\"mixed\")\n","        df = df.dropna(subset=[\"Date\"])\n","\n","        end_date = pd.to_datetime(CONFIG[\"end_date\"]) + timedelta(days=1)\n","        mask = (df[\"Date\"] >= pd.to_datetime(CONFIG[\"start_date\"])) & (df[\"Date\"] <= end_date)\n","        df = df.loc[mask]\n","\n","        if df.empty:\n","            print(f\"Uyarı: {os.path.basename(file_path)} dosyası filtreleme sonrası boş\")\n","            return pd.DataFrame()\n","\n","        for col in [\"Close\", \"High\", \"Low\", \"Open\", \"Volume\"]:\n","            df[col] = pd.to_numeric(df[col], errors='coerce')\n","\n","        return df.dropna()\n","\n","    except Exception as e:\n","        print(f\"Hata: {file_path} yüklenirken - {str(e)}\")\n","        return pd.DataFrame()\n","\n","def load_company_data(company_code: str) -> Tuple[pd.DataFrame, pd.DataFrame]:\n","    train_path = f\"Datas/{company_code}.csv\"\n","    test_path = f\"Test/{company_code}.csv\"\n","\n","    train_df = load_and_preprocess_single(train_path)\n","    test_df = load_and_preprocess_single(test_path)\n","\n","    if train_df.empty or test_df.empty:\n","        return pd.DataFrame(), pd.DataFrame()\n","\n","    def calculate_features(df):\n","        df = df.copy()\n","        df[\"Change\"] = df[\"Close\"].pct_change(fill_method=None) * 100\n","\n","        # Özellik mühendisliği\n","        df[\"MA_20\"] = df[\"Close\"].rolling(20, min_periods=1).mean()\n","\n","        # RSI hesaplama\n","        rsi = ta.momentum.RSIIndicator(df[\"Close\"], window=14).rsi()\n","        df[\"RSI\"] = rsi.fillna(50)\n","\n","        # Volatilite\n","        volatility = df[\"Close\"].pct_change().rolling(5, min_periods=1).std()\n","        df[\"Volatility\"] = volatility.fillna(0.01)\n","\n","        # Önceki günün kapanışı\n","        df[\"Prev_Close\"] = df[\"Close\"].shift(1)\n","\n","        return df.dropna()\n","\n","    train_df = calculate_features(train_df)\n","    test_df = calculate_features(test_df)\n","\n","    return train_df, test_df\n","\n","def dater(df_train: pd.DataFrame, df_test: pd.DataFrame) -> tuple:\n","    def process_date_features(df):\n","        df = df.copy()\n","        df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n","        df[\"Year\"] = df[\"Date\"].dt.year\n","        df[\"Month\"] = df[\"Date\"].dt.month\n","        df[\"Day\"] = df[\"Date\"].dt.day\n","        df[\"Weekday\"] = df[\"Date\"].dt.weekday\n","        return df\n","\n","    df_train = process_date_features(df_train)\n","    df_test = process_date_features(df_test)\n","\n","    # Hedef değişken ve özellikler\n","    y_train = df_train[\"Change\"]\n","    # Gereksiz sütunları kaldır\n","    X_train = df_train.drop([\"Date\", \"Change\", \"CDirection\"], axis=1, errors=\"ignore\")\n","\n","    actual_change = df_test[\"Change\"].reset_index(drop=True)\n","    test_dates = df_test[\"Date\"].reset_index(drop=True)\n","    X_test = df_test.drop([\"Date\", \"Change\", \"CDirection\"], axis=1, errors=\"ignore\")\n","\n","    return X_train, y_train, actual_change, test_dates, X_test\n","\n","def DRpredictor(company_code: str) -> pd.DataFrame:\n","    train_df, test_df = load_company_data(company_code)\n","\n","    if train_df.empty or test_df.empty:\n","        print(f\"{company_code} için yeterli veri yok. Atlanıyor.\")\n","        return pd.DataFrame()\n","\n","    X_train, y_train, actual_change, test_dates, X_test = dater(train_df, test_df)\n","\n","    # Hacim sütununu log dönüşümü\n","    for df in [X_train, X_test]:\n","        if \"Volume\" in df.columns:\n","            df[\"Volume\"] = np.log1p(df[\"Volume\"])\n","\n","    tscv = TimeSeriesSplit(n_splits=CONFIG[\"n_splits\"])\n","    best_val_loss = float('inf')\n","    best_scaler = None\n","    best_model = None\n","\n","    for fold, (train_idx, val_idx) in enumerate(tscv.split(X_train), 1):\n","        X_t, X_v = X_train.iloc[train_idx], X_train.iloc[val_idx]\n","        y_t, y_v = y_train.iloc[train_idx], y_train.iloc[val_idx]\n","\n","        scaler = StandardScaler()\n","        X_t_scaled = scaler.fit_transform(X_t)\n","        X_v_scaled = scaler.transform(X_v)\n","\n","        model = SGDRegressor(\n","            alpha=0.0001,\n","            learning_rate='invscaling',\n","            eta0=0.001,\n","            max_iter=1000,\n","            tol=1e-3,\n","            random_state=42,\n","            early_stopping=False\n","        )\n","        model.fit(X_t_scaled, y_t)\n","\n","        y_pred_val = model.predict(X_v_scaled)\n","        val_loss = mean_squared_error(y_v, y_pred_val)\n","\n","        if val_loss < best_val_loss:\n","            best_val_loss = val_loss\n","            best_scaler = scaler\n","            best_model = model\n","\n","    print(f\"\\nEn iyi model val_loss (MSE): {best_val_loss:.6f}\")\n","\n","    # 1. BEST_MODEL'İ TÜM EĞİTİM VERİSİYLE YENİDEN EĞİT\n","    X_train_full_scaled = best_scaler.transform(X_train)\n","    best_model_full = SGDRegressor(\n","        **best_model.get_params()  # Aynı hiperparametreleri kullan\n","    )\n","    best_model_full.fit(X_train_full_scaled, y_train)\n","\n","    # 2. DEEPCOPY İLE DYNAMIC_MODEL OLUŞTUR\n","    dynamic_model = copy.deepcopy(best_model_full)\n","    dynamic_model.warm_start = True  # Artımlı öğrenme için\n","\n","    # Sonuçları saklamak için yapı\n","    results = {\n","        \"Date\": [],\n","        \"Real_Change\": [],\n","        \"Predicted_Change\": [],\n","        \"Squared_Error\": [],\n","        \"Cumulative_MSE\": []\n","    }\n","\n","    total_squared_error = 0\n","    X_walk = X_train.copy()\n","    y_walk = y_train.copy()\n","\n","    # 3. İLK DURUMDA DYNAMIC_MODEL ZATEN EĞİTİLMİŞ DURUMDA\n","    for i in range(len(X_test)):\n","        x_i = X_test.iloc[[i]]\n","        x_i_scaled = best_scaler.transform(x_i)\n","\n","        # DYNAMIC_MODEL ile tahmin yap\n","        y_pred = dynamic_model.predict(x_i_scaled)[0]\n","        y_true = actual_change.iloc[i]\n","        current_date = test_dates.iloc[i]\n","\n","        squared_error = (y_true - y_pred) ** 2\n","        total_squared_error += squared_error\n","        cumulative_mse = total_squared_error / (i + 1)\n","\n","        results[\"Date\"].append(current_date)\n","        results[\"Real_Change\"].append(y_true)\n","        results[\"Predicted_Change\"].append(y_pred)\n","        results[\"Squared_Error\"].append(squared_error)\n","        results[\"Cumulative_MSE\"].append(cumulative_mse)\n","\n","        # Modeli güncelle\n","        X_walk = pd.concat([X_walk, x_i], ignore_index=True)\n","        y_walk = pd.concat([y_walk, pd.Series([y_true])], ignore_index=True)\n","\n","        # Her 5 adımda bir modeli yeniden eğit\n","        if (i + 1) % 5 == 0:\n","            X_walk_scaled = best_scaler.transform(X_walk)\n","            dynamic_model.fit(X_walk_scaled, y_walk)\n","\n","    result_df = pd.DataFrame(results)\n","    print(f\"\\n{company_code} için walk-forward tamamlandı. Son MSE: {cumulative_mse:.6f}\")\n","    return result_df'''"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":192},"id":"xJbFcs2FsGLe","executionInfo":{"status":"ok","timestamp":1749993166731,"user_tz":-180,"elapsed":55,"user":{"displayName":"Osman Mango","userId":"07182088882667002738"}},"outputId":"46c27058-5190-4ecd-ce8c-9ed267512fc5"},"id":"xJbFcs2FsGLe","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'CONFIG = {\\n    \"window_size\": 64,\\n    \"test_size\": 64,\\n    \"data_paths\": [\"Datas/*.csv\", \"Test/*.csv\"],\\n    \"start_date\": \"2006-07-27\",\\n    \"end_date\": \"2025-05-06\",\\n    \"n_splits\": 15,\\n}\\n\\ndef load_and_preprocess_single(file_path: str) -> pd.DataFrame:\\n    try:\\n        df = pd.read_csv(\\n            file_path,\\n            skiprows=2,\\n            header=None,\\n            names=[\"Date\", \"Close\", \"High\", \"Low\", \"Open\", \"Volume\"]\\n        )\\n\\n        df[\"Date\"] = pd.to_datetime(df[\"Date\"], errors=\\'coerce\\', format=\"mixed\")\\n        df = df.dropna(subset=[\"Date\"])\\n\\n        end_date = pd.to_datetime(CONFIG[\"end_date\"]) + timedelta(days=1)\\n        mask = (df[\"Date\"] >= pd.to_datetime(CONFIG[\"start_date\"])) & (df[\"Date\"] <= end_date)\\n        df = df.loc[mask]\\n\\n        if df.empty:\\n            print(f\"Uyarı: {os.path.basename(file_path)} dosyası filtreleme sonrası boş\")\\n            return pd.DataFrame()\\n\\n        for col in [\"Close\", \"High\", \"Low\", \"Open\", \"Volume\"]:\\n            df[col] = pd.to_numeric(df[col], errors=\\'coerce\\')\\n\\n        return df.dropna()\\n\\n    except Exception as e:\\n        print(f\"Hata: {file_path} yüklenirken - {str(e)}\")\\n        return pd.DataFrame()\\n\\ndef load_company_data(company_code: str) -> Tuple[pd.DataFrame, pd.DataFrame]:\\n    train_path = f\"Datas/{company_code}.csv\"\\n    test_path = f\"Test/{company_code}.csv\"\\n\\n    train_df = load_and_preprocess_single(train_path)\\n    test_df = load_and_preprocess_single(test_path)\\n\\n    if train_df.empty or test_df.empty:\\n        return pd.DataFrame(), pd.DataFrame()\\n\\n    def calculate_features(df):\\n        df = df.copy()\\n        df[\"Change\"] = df[\"Close\"].pct_change(fill_method=None) * 100\\n        \\n        # Özellik mühendisliği\\n        df[\"MA_20\"] = df[\"Close\"].rolling(20, min_periods=1).mean()\\n        \\n        # RSI hesaplama\\n        rsi = ta.momentum.RSIIndicator(df[\"Close\"], window=14).rsi()\\n        df[\"RSI\"] = rsi.fillna(50)\\n        \\n        # Volatilite\\n        volatility = df[\"Close\"].pct_change().rolling(5, min_periods=1).std()\\n        df[\"Volatility\"] = volatility.fillna(0.01)\\n        \\n        # Önceki günün kapanışı\\n        df[\"Prev_Close\"] = df[\"Close\"].shift(1)\\n        \\n        return df.dropna()\\n\\n    train_df = calculate_features(train_df)\\n    test_df = calculate_features(test_df)\\n\\n    return train_df, test_df\\n\\ndef dater(df_train: pd.DataFrame, df_test: pd.DataFrame) -> tuple:\\n    def process_date_features(df):\\n        df = df.copy()\\n        df[\"Date\"] = pd.to_datetime(df[\"Date\"])\\n        df[\"Year\"] = df[\"Date\"].dt.year\\n        df[\"Month\"] = df[\"Date\"].dt.month\\n        df[\"Day\"] = df[\"Date\"].dt.day\\n        df[\"Weekday\"] = df[\"Date\"].dt.weekday\\n        return df\\n\\n    df_train = process_date_features(df_train)\\n    df_test = process_date_features(df_test)\\n\\n    # Hedef değişken ve özellikler\\n    y_train = df_train[\"Change\"]\\n    # Gereksiz sütunları kaldır\\n    X_train = df_train.drop([\"Date\", \"Change\", \"CDirection\"], axis=1, errors=\"ignore\")\\n    \\n    actual_change = df_test[\"Change\"].reset_index(drop=True)\\n    test_dates = df_test[\"Date\"].reset_index(drop=True)\\n    X_test = df_test.drop([\"Date\", \"Change\", \"CDirection\"], axis=1, errors=\"ignore\")\\n\\n    return X_train, y_train, actual_change, test_dates, X_test\\n\\ndef DRpredictor(company_code: str) -> pd.DataFrame:\\n    train_df, test_df = load_company_data(company_code)\\n\\n    if train_df.empty or test_df.empty:\\n        print(f\"{company_code} için yeterli veri yok. Atlanıyor.\")\\n        return pd.DataFrame()\\n\\n    X_train, y_train, actual_change, test_dates, X_test = dater(train_df, test_df)\\n\\n    # Hacim sütununu log dönüşümü\\n    for df in [X_train, X_test]:\\n        if \"Volume\" in df.columns:\\n            df[\"Volume\"] = np.log1p(df[\"Volume\"])\\n    \\n    tscv = TimeSeriesSplit(n_splits=CONFIG[\"n_splits\"])\\n    best_val_loss = float(\\'inf\\')\\n    best_scaler = None\\n    best_model = None\\n\\n    for fold, (train_idx, val_idx) in enumerate(tscv.split(X_train), 1):\\n        X_t, X_v = X_train.iloc[train_idx], X_train.iloc[val_idx]\\n        y_t, y_v = y_train.iloc[train_idx], y_train.iloc[val_idx]\\n\\n        scaler = StandardScaler()\\n        X_t_scaled = scaler.fit_transform(X_t)\\n        X_v_scaled = scaler.transform(X_v)\\n\\n        model = SGDRegressor(\\n            alpha=0.0001,\\n            learning_rate=\\'invscaling\\',\\n            eta0=0.001,\\n            max_iter=1000,\\n            tol=1e-3,\\n            random_state=42,\\n            early_stopping=False\\n        )\\n        model.fit(X_t_scaled, y_t)\\n\\n        y_pred_val = model.predict(X_v_scaled)\\n        val_loss = mean_squared_error(y_v, y_pred_val)\\n\\n        if val_loss < best_val_loss:\\n            best_val_loss = val_loss\\n            best_scaler = scaler\\n            best_model = model\\n\\n    print(f\"\\nEn iyi model val_loss (MSE): {best_val_loss:.6f}\")\\n\\n    # 1. BEST_MODEL\\'İ TÜM EĞİTİM VERİSİYLE YENİDEN EĞİT\\n    X_train_full_scaled = best_scaler.transform(X_train)\\n    best_model_full = SGDRegressor(\\n        **best_model.get_params()  # Aynı hiperparametreleri kullan\\n    )\\n    best_model_full.fit(X_train_full_scaled, y_train)\\n    \\n    # 2. DEEPCOPY İLE DYNAMIC_MODEL OLUŞTUR\\n    dynamic_model = copy.deepcopy(best_model_full)\\n    dynamic_model.warm_start = True  # Artımlı öğrenme için\\n\\n    # Sonuçları saklamak için yapı\\n    results = {\\n        \"Date\": [],\\n        \"Real_Change\": [],\\n        \"Predicted_Change\": [],\\n        \"Squared_Error\": [],\\n        \"Cumulative_MSE\": []\\n    }\\n\\n    total_squared_error = 0\\n    X_walk = X_train.copy()\\n    y_walk = y_train.copy()\\n\\n    # 3. İLK DURUMDA DYNAMIC_MODEL ZATEN EĞİTİLMİŞ DURUMDA\\n    for i in range(len(X_test)):\\n        x_i = X_test.iloc[[i]]\\n        x_i_scaled = best_scaler.transform(x_i)\\n\\n        # DYNAMIC_MODEL ile tahmin yap\\n        y_pred = dynamic_model.predict(x_i_scaled)[0]\\n        y_true = actual_change.iloc[i]\\n        current_date = test_dates.iloc[i]\\n\\n        squared_error = (y_true - y_pred) ** 2\\n        total_squared_error += squared_error\\n        cumulative_mse = total_squared_error / (i + 1)\\n\\n        results[\"Date\"].append(current_date)\\n        results[\"Real_Change\"].append(y_true)\\n        results[\"Predicted_Change\"].append(y_pred)\\n        results[\"Squared_Error\"].append(squared_error)\\n        results[\"Cumulative_MSE\"].append(cumulative_mse)\\n\\n        # Modeli güncelle\\n        X_walk = pd.concat([X_walk, x_i], ignore_index=True)\\n        y_walk = pd.concat([y_walk, pd.Series([y_true])], ignore_index=True)\\n\\n        # Her 5 adımda bir modeli yeniden eğit\\n        if (i + 1) % 5 == 0:\\n            X_walk_scaled = best_scaler.transform(X_walk)\\n            dynamic_model.fit(X_walk_scaled, y_walk)\\n\\n    result_df = pd.DataFrame(results)\\n    print(f\"\\n{company_code} için walk-forward tamamlandı. Son MSE: {cumulative_mse:.6f}\")\\n    return result_df'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["'''if __name__ == \"__main__\":\n","    result_dir = \"Results/Change/Beast\"\n","    os.makedirs(result_dir, exist_ok=True)\n","\n","    company_files = sorted(glob.glob(\"Datas/*.csv\"))\n","    print(f\"İşlenecek toplam şirket sayısı: {len(company_files)}\")\n","\n","    for idx, filepath in enumerate(company_files):\n","        company_code = os.path.splitext(os.path.basename(filepath))[0]\n","        print(f\"\\n[{idx+1}/{len(company_files)}] {company_code} işleniyor...\")\n","\n","        try:\n","            result_df = DRpredictor(company_code)\n","\n","            if not result_df.empty:\n","                result_path = os.path.join(result_dir, f\"{company_code}_results.csv\")\n","                result_df.to_csv(result_path, index=False)\n","                print(f\"→ Sonuçlar kaydedildi: {result_path}\")\n","        except Exception as e:\n","            print(f\"{company_code} işlenirken hata: {str(e)}\")'''"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":157},"id":"3IBOUH4asWtW","executionInfo":{"status":"ok","timestamp":1749993172217,"user_tz":-180,"elapsed":14,"user":{"displayName":"Osman Mango","userId":"07182088882667002738"}},"outputId":"df7d9820-3736-4840-e3e8-729ca2435b9c"},"id":"3IBOUH4asWtW","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'if __name__ == \"__main__\":\\n    result_dir = \"Results/Change/Beast\"\\n    os.makedirs(result_dir, exist_ok=True)\\n\\n    company_files = sorted(glob.glob(\"Datas/*.csv\"))\\n    print(f\"İşlenecek toplam şirket sayısı: {len(company_files)}\")\\n\\n    for idx, filepath in enumerate(company_files):\\n        company_code = os.path.splitext(os.path.basename(filepath))[0]\\n        print(f\"\\n[{idx+1}/{len(company_files)}] {company_code} işleniyor...\")\\n\\n        try:\\n            result_df = DRpredictor(company_code)\\n\\n            if not result_df.empty:\\n                result_path = os.path.join(result_dir, f\"{company_code}_results.csv\")\\n                result_df.to_csv(result_path, index=False)\\n                print(f\"→ Sonuçlar kaydedildi: {result_path}\")\\n        except Exception as e:\\n            print(f\"{company_code} işlenirken hata: {str(e)}\")'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["from typing_extensions import final\n","CONFIG = {\n","    \"window_size\": 64,\n","    \"test_size\": 64,\n","    \"data_paths\": [\"Datas/*.csv\", \"Test/*.csv\"],\n","    \"start_date\": \"2006-07-27\",\n","    \"end_date\": \"2025-05-06\",\n","    \"n_splits\": 12,\n","}\n","\n","def load_and_preprocess_single(file_path: str) -> pd.DataFrame:\n","    try:\n","        df = pd.read_csv(\n","            file_path,\n","            skiprows=2,\n","            header=None,\n","            names=[\"Date\", \"Close\", \"High\", \"Low\", \"Open\", \"Volume\"]\n","        )\n","\n","        df[\"Date\"] = pd.to_datetime(df[\"Date\"], errors='coerce', format='mixed')\n","        df = df.dropna(subset=[\"Date\"])\n","\n","        end_date = pd.to_datetime(CONFIG[\"end_date\"]) + timedelta(days=1)\n","        mask = (df[\"Date\"] >= pd.to_datetime(CONFIG[\"start_date\"])) & (df[\"Date\"] <= end_date)\n","        df = df.loc[mask]\n","\n","        if df.empty:\n","            print(f\"Uyarı: {os.path.basename(file_path)} dosyası filtreleme sonrası boş\")\n","            return pd.DataFrame()\n","\n","        for col in [\"Close\", \"High\", \"Low\", \"Open\", \"Volume\"]:\n","            df[col] = pd.to_numeric(df[col], errors='coerce')\n","\n","        return df.dropna()\n","\n","    except Exception as e:\n","        print(f\"Hata: {file_path} yüklenirken - {str(e)}\")\n","        return pd.DataFrame()\n","\n","def load_company_data(company_code: str) -> Tuple[pd.DataFrame, pd.DataFrame]:\n","    train_path = f\"Datas/{company_code}.csv\"\n","    test_path = f\"Test/{company_code}.csv\"\n","\n","    train_df = load_and_preprocess_single(train_path)\n","    test_df = load_and_preprocess_single(test_path)\n","\n","    if train_df.empty or test_df.empty:\n","        return pd.DataFrame(), pd.DataFrame()\n","\n","    def calculate_features(df):\n","        df = df.copy()\n","        df[\"Change\"] = df[\"Close\"].pct_change(fill_method=None) * 100\n","\n","        # Özellik mühendisliği\n","        df[\"MA_20\"] = df[\"Close\"].rolling(20, min_periods=1).mean()\n","\n","        # RSI hesaplama\n","        rsi = ta.momentum.RSIIndicator(df[\"Close\"], window=14).rsi()\n","        df[\"RSI\"] = rsi.fillna(50)\n","\n","        # Volatilite\n","        volatility = df[\"Close\"].pct_change().rolling(5, min_periods=1).std()\n","        df[\"Volatility\"] = volatility.fillna(0.01)\n","\n","        return df.dropna()\n","\n","    train_df = calculate_features(train_df)\n","    test_df = calculate_features(test_df)\n","\n","    return train_df, test_df\n","\n","def dater(df_train: pd.DataFrame, df_test: pd.DataFrame) -> tuple:\n","    def process_date_features(df):\n","        df = df.copy()\n","        df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n","        df[\"Year\"] = df[\"Date\"].dt.year\n","        df[\"Month\"] = df[\"Date\"].dt.month\n","        df[\"Day\"] = df[\"Date\"].dt.day\n","        df[\"Weekday\"] = df[\"Date\"].dt.weekday\n","        return df\n","\n","    df_train = process_date_features(df_train)\n","    df_test = process_date_features(df_test)\n","\n","    y_train = df_train[\"Change\"]\n","    X_train = df_train.drop([\"Date\", \"Change\", \"CDirection\"], axis=1, errors=\"ignore\")\n","\n","    actual_change = df_test[\"Change\"].reset_index(drop=True)\n","    test_dates = df_test[\"Date\"].reset_index(drop=True)\n","    X_test = df_test.drop([\"Date\", \"Change\", \"CDirection\"], axis=1, errors=\"ignore\")\n","\n","    return X_train, y_train, actual_change, test_dates, X_test\n","\n","def DRpredictor(company_code: str) -> pd.DataFrame:\n","    train_df, test_df = load_company_data(company_code)\n","\n","    if train_df.empty or test_df.empty:\n","        print(f\"{company_code} için yeterli veri yok. Atlanıyor.\")\n","        return pd.DataFrame()\n","\n","    X_train, y_train, actual_change, test_dates, X_test = dater(train_df, test_df)\n","\n","    scaler = RobustScaler(quantile_range=(5, 95))\n","    X_train_scaled = scaler.fit_transform(X_train)\n","    X_test_scaled = scaler.transform(X_test)\n","\n","\n","    tscv = TimeSeriesSplit(n_splits=CONFIG[\"n_splits\"])\n","    best_val_loss = float('inf')\n","    best_model = None\n","\n","    for fold, (train_idx, val_idx) in enumerate(tscv.split(X_train_scaled), 1):\n","        X_t, X_v = X_train_scaled[train_idx], X_train_scaled[val_idx]\n","        y_t, y_v = y_train.iloc[train_idx], y_train.iloc[val_idx]\n","\n","        model = HuberRegressor(\n","            alpha=0.001,\n","            epsilon=1.5,\n","            max_iter=1000,\n","            tol=1e-4\n","        )\n","        model.fit(X_t, y_t)\n","\n","        y_pred_val = model.predict(X_v)\n","        val_loss = mean_squared_error(y_v, y_pred_val)\n","\n","        if val_loss < best_val_loss:\n","            best_val_loss = val_loss\n","            best_model = model\n","\n","    print(f\"\\nEn iyi model val_loss (MSE): {best_val_loss:.6f}\")\n","\n","    best_model.fit(X_train_scaled, y_train)\n","\n","    results = {\n","        \"Date\": [],\n","        \"Real_Change\": [],\n","        \"Predicted_Change\": [],\n","        \"Squared_Error\": [],\n","        \"Cumulative_MSE\": []\n","    }\n","\n","    total_squared_error = 0\n","    X_walk = X_train_scaled.copy()\n","    y_walk = y_train.copy().values\n","\n","    for i in range(len(X_test_scaled)):\n","        x_i = X_test_scaled[i].reshape(1, -1)\n","\n","        y_pred = best_model.predict(x_i)[0]\n","        y_true = actual_change.iloc[i]\n","        current_date = test_dates.iloc[i]\n","\n","        squared_error = (y_true - y_pred) ** 2\n","        total_squared_error += squared_error\n","        cumulative_mse = total_squared_error / (i + 1)\n","\n","        results[\"Date\"].append(current_date)\n","        results[\"Real_Change\"].append(y_true)\n","        results[\"Predicted_Change\"].append(y_pred)\n","        results[\"Squared_Error\"].append(squared_error)\n","        results[\"Cumulative_MSE\"].append(cumulative_mse)\n","\n","        X_walk = np.vstack([X_walk, x_i])\n","        y_walk = np.append(y_walk, y_true)\n","\n","        best_model.fit(X_walk, y_walk)\n","\n","    result_df = pd.DataFrame(results)\n","    final_mse = cumulative_mse\n","    print(f\"\\n{company_code} için walk-forward tamamlandı. Son MSE: {final_mse:.6f}\")\n","\n","    return result_df"],"metadata":{"id":"zM0sVvg9xyO1","executionInfo":{"status":"ok","timestamp":1750000655245,"user_tz":-180,"elapsed":39,"user":{"displayName":"Osman Mango","userId":"07182088882667002738"}}},"id":"zM0sVvg9xyO1","execution_count":22,"outputs":[]},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","    result_dir = \"Results/Change/Beast\"\n","    os.makedirs(result_dir, exist_ok=True)\n","\n","    company_files = sorted(glob.glob(\"Datas/*.csv\"))\n","    print(f\"İşlenecek toplam şirket sayısı: {len(company_files)}\")\n","\n","    for idx, filepath in enumerate(company_files):\n","        company_code = os.path.splitext(os.path.basename(filepath))[0]\n","        print(f\"\\n[{idx+1}/{len(company_files)}] {company_code} işleniyor...\")\n","\n","        try:\n","            result_df = DRpredictor(company_code)\n","\n","            if not result_df.empty:\n","                result_path = os.path.join(result_dir, f\"{company_code}_results.csv\")\n","                result_df.to_csv(result_path, index=False)\n","                print(f\"→ Sonuçlar kaydedildi: {result_path}\")\n","        except Exception as e:\n","            print(f\"{company_code} işlenirken hata: {str(e)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-fAA5yXPx0uX","executionInfo":{"status":"ok","timestamp":1750001183850,"user_tz":-180,"elapsed":525676,"user":{"displayName":"Osman Mango","userId":"07182088882667002738"}},"outputId":"0fd71f62-088d-4d1a-c03c-acb7738d392b"},"id":"-fAA5yXPx0uX","execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["İşlenecek toplam şirket sayısı: 30\n","\n","[1/30] AKBNK işleniyor...\n","\n","En iyi model val_loss (MSE): 0.651192\n","\n","AKBNK için walk-forward tamamlandı. Son MSE: 3.258560\n","→ Sonuçlar kaydedildi: Results/Change/Beast/AKBNK_results.csv\n","\n","[2/30] ALARK işleniyor...\n","\n","En iyi model val_loss (MSE): 2.142265\n","\n","ALARK için walk-forward tamamlandı. Son MSE: 2.562204\n","→ Sonuçlar kaydedildi: Results/Change/Beast/ALARK_results.csv\n","\n","[3/30] ARCLK işleniyor...\n","\n","En iyi model val_loss (MSE): 0.727653\n","\n","ARCLK için walk-forward tamamlandı. Son MSE: 1.796371\n","→ Sonuçlar kaydedildi: Results/Change/Beast/ARCLK_results.csv\n","\n","[4/30] ASELS işleniyor...\n","\n","En iyi model val_loss (MSE): 1.345815\n","\n","ASELS için walk-forward tamamlandı. Son MSE: 10.327622\n","→ Sonuçlar kaydedildi: Results/Change/Beast/ASELS_results.csv\n","\n","[5/30] BIMAS işleniyor...\n","\n","En iyi model val_loss (MSE): 0.969401\n","\n","BIMAS için walk-forward tamamlandı. Son MSE: 2.328013\n","→ Sonuçlar kaydedildi: Results/Change/Beast/BIMAS_results.csv\n","\n","[6/30] BRSAN işleniyor...\n","\n","En iyi model val_loss (MSE): 2.857552\n","\n","BRSAN için walk-forward tamamlandı. Son MSE: 2.352771\n","→ Sonuçlar kaydedildi: Results/Change/Beast/BRSAN_results.csv\n","\n","[7/30] EKGYO işleniyor...\n","\n","En iyi model val_loss (MSE): 0.652125\n","\n","EKGYO için walk-forward tamamlandı. Son MSE: 5.141423\n","→ Sonuçlar kaydedildi: Results/Change/Beast/EKGYO_results.csv\n","\n","[8/30] EREGL işleniyor...\n","\n","En iyi model val_loss (MSE): 1.463776\n","\n","EREGL için walk-forward tamamlandı. Son MSE: 2.030549\n","→ Sonuçlar kaydedildi: Results/Change/Beast/EREGL_results.csv\n","\n","[9/30] FROTO işleniyor...\n","\n","En iyi model val_loss (MSE): 1.874641\n","\n","FROTO için walk-forward tamamlandı. Son MSE: 14.118924\n","→ Sonuçlar kaydedildi: Results/Change/Beast/FROTO_results.csv\n","\n","[10/30] GARAN işleniyor...\n","\n","En iyi model val_loss (MSE): 0.998157\n","\n","GARAN için walk-forward tamamlandı. Son MSE: 4.289319\n","→ Sonuçlar kaydedildi: Results/Change/Beast/GARAN_results.csv\n","\n","[11/30] GUBRF işleniyor...\n","\n","En iyi model val_loss (MSE): 2.421703\n","\n","GUBRF için walk-forward tamamlandı. Son MSE: 1.852126\n","→ Sonuçlar kaydedildi: Results/Change/Beast/GUBRF_results.csv\n","\n","[12/30] HEKTS işleniyor...\n","\n","En iyi model val_loss (MSE): 1.429091\n","\n","HEKTS için walk-forward tamamlandı. Son MSE: 4.415928\n","→ Sonuçlar kaydedildi: Results/Change/Beast/HEKTS_results.csv\n","\n","[13/30] KCHOL işleniyor...\n","\n","En iyi model val_loss (MSE): 1.572169\n","\n","KCHOL için walk-forward tamamlandı. Son MSE: 2.352492\n","→ Sonuçlar kaydedildi: Results/Change/Beast/KCHOL_results.csv\n","\n","[14/30] KOZAA işleniyor...\n","\n","En iyi model val_loss (MSE): 1.666401\n","\n","KOZAA için walk-forward tamamlandı. Son MSE: 3.455701\n","→ Sonuçlar kaydedildi: Results/Change/Beast/KOZAA_results.csv\n","\n","[15/30] KOZAL işleniyor...\n","\n","En iyi model val_loss (MSE): 1.594152\n","\n","KOZAL için walk-forward tamamlandı. Son MSE: 2.136177\n","→ Sonuçlar kaydedildi: Results/Change/Beast/KOZAL_results.csv\n","\n","[16/30] KRDMD işleniyor...\n","\n","En iyi model val_loss (MSE): 2.633332\n","\n","KRDMD için walk-forward tamamlandı. Son MSE: 2.908270\n","→ Sonuçlar kaydedildi: Results/Change/Beast/KRDMD_results.csv\n","\n","[17/30] MGROS işleniyor...\n","\n","En iyi model val_loss (MSE): 1.695521\n","\n","MGROS için walk-forward tamamlandı. Son MSE: 2.287246\n","→ Sonuçlar kaydedildi: Results/Change/Beast/MGROS_results.csv\n","\n","[18/30] PETKM işleniyor...\n","\n","En iyi model val_loss (MSE): 0.902572\n","\n","PETKM için walk-forward tamamlandı. Son MSE: 1.022566\n","→ Sonuçlar kaydedildi: Results/Change/Beast/PETKM_results.csv\n","\n","[19/30] PGSUS işleniyor...\n","\n","En iyi model val_loss (MSE): 3.076049\n","\n","PGSUS için walk-forward tamamlandı. Son MSE: 3.534451\n","→ Sonuçlar kaydedildi: Results/Change/Beast/PGSUS_results.csv\n","\n","[20/30] SAHOL işleniyor...\n","\n","En iyi model val_loss (MSE): 0.843669\n","\n","SAHOL için walk-forward tamamlandı. Son MSE: 2.445943\n","→ Sonuçlar kaydedildi: Results/Change/Beast/SAHOL_results.csv\n","\n","[21/30] SASA işleniyor...\n","\n","En iyi model val_loss (MSE): 1.349674\n","\n","SASA için walk-forward tamamlandı. Son MSE: 1.860915\n","→ Sonuçlar kaydedildi: Results/Change/Beast/SASA_results.csv\n","\n","[22/30] SISE işleniyor...\n","\n","En iyi model val_loss (MSE): 0.910527\n","\n","SISE için walk-forward tamamlandı. Son MSE: 1.431827\n","→ Sonuçlar kaydedildi: Results/Change/Beast/SISE_results.csv\n","\n","[23/30] SOKM işleniyor...\n","\n","En iyi model val_loss (MSE): 0.827838\n","\n","SOKM için walk-forward tamamlandı. Son MSE: 2.363620\n","→ Sonuçlar kaydedildi: Results/Change/Beast/SOKM_results.csv\n","\n","[24/30] TCELL işleniyor...\n","\n","En iyi model val_loss (MSE): 0.760108\n","\n","TCELL için walk-forward tamamlandı. Son MSE: 3.884716\n","→ Sonuçlar kaydedildi: Results/Change/Beast/TCELL_results.csv\n","\n","[25/30] THYAO işleniyor...\n","\n","En iyi model val_loss (MSE): 3.415467\n","\n","THYAO için walk-forward tamamlandı. Son MSE: 1.633432\n","→ Sonuçlar kaydedildi: Results/Change/Beast/THYAO_results.csv\n","\n","[26/30] TKFEN işleniyor...\n","\n","En iyi model val_loss (MSE): 1.613580\n","\n","TKFEN için walk-forward tamamlandı. Son MSE: 89.398561\n","→ Sonuçlar kaydedildi: Results/Change/Beast/TKFEN_results.csv\n","\n","[27/30] TOASO işleniyor...\n","\n","En iyi model val_loss (MSE): 1.400826\n","\n","TOASO için walk-forward tamamlandı. Son MSE: 2.331726\n","→ Sonuçlar kaydedildi: Results/Change/Beast/TOASO_results.csv\n","\n","[28/30] TUPRS işleniyor...\n","\n","En iyi model val_loss (MSE): 1.830891\n","\n","TUPRS için walk-forward tamamlandı. Son MSE: 1.467347\n","→ Sonuçlar kaydedildi: Results/Change/Beast/TUPRS_results.csv\n","\n","[29/30] VAKBN işleniyor...\n","\n","En iyi model val_loss (MSE): 1.178002\n","\n","VAKBN için walk-forward tamamlandı. Son MSE: 6.781336\n","→ Sonuçlar kaydedildi: Results/Change/Beast/VAKBN_results.csv\n","\n","[30/30] YKBNK işleniyor...\n","\n","En iyi model val_loss (MSE): 1.201170\n","\n","YKBNK için walk-forward tamamlandı. Son MSE: 2.170470\n","→ Sonuçlar kaydedildi: Results/Change/Beast/YKBNK_results.csv\n"]}]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.17"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}